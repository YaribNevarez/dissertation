\begin{thebibliography}{100}

\bibitem{lasi2014industry}
Heiner Lasi, Peter Fettke, Hans-Georg Kemper, Thomas Feld, and Michael
  Hoffmann.
\newblock Industry 4.0.
\newblock {\em Business \& information systems engineering}, 6(4):239--242,
  2014.

\bibitem{espinoza2020estimating}
H{\'e}ctor Espinoza, Gerhard Kling, Frank McGroarty, Mary O'Mahony, and Xenia
  Ziouvelou.
\newblock Estimating the impact of the internet of things on productivity in
  europe.
\newblock {\em Heliyon}, 6(5):e03935, 2020.

\bibitem{alcacer2019scanning}
Vitor Alc{\'a}cer and Virgilio Cruz-Machado.
\newblock Scanning the industry 4.0: A literature review on technologies for
  manufacturing systems.
\newblock {\em Engineering science and technology, an international journal},
  22(3):899--919, 2019.

\bibitem{zhang2020empowering}
Jing Zhang and Dacheng Tao.
\newblock Empowering things with intelligence: A survey of the progress,
  challenges, and opportunities in artificial intelligence of things.
\newblock {\em IEEE Internet of Things Journal}, 2020.

\bibitem{loh20201}
Kou-Hung~Lawrence Loh.
\newblock 1.2 fertilizing aiot from roots to leaves.
\newblock In {\em 2020 IEEE International Solid-State Circuits
  Conference-(ISSCC)}, pages 15--21. IEEE, 2020.

\bibitem{han2013approximate}
Jie Han and Michael Orshansky.
\newblock Approximate computing: An emerging paradigm for energy-efficient
  design.
\newblock In {\em 2013 18th IEEE European Test Symposium (ETS)}, pages 1--6.
  IEEE, 2013.

\bibitem{bouvier2019spiking}
Maxence Bouvier, Alexandre Valentian, Thomas Mesquida, Fran{\c{c}}ois Rummens,
  Marina Reyboz, Elisa Vianello, and Edith Beigne.
\newblock Spiking neural networks hardware implementations and challenges: A
  survey.
\newblock {\em ACM Journal on Emerging Technologies in Computing Systems
  (JETC)}, 15(2):1--35, 2019.

\bibitem{courbariaux2015binaryconnect}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In {\em Advances in neural information processing systems}, pages
  3123--3131, 2015.

\bibitem{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock {\em arXiv preprint arXiv:1510.00149}, 2015.

\bibitem{hubara2017quantized}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Quantized neural networks: Training neural networks with low
  precision weights and activations.
\newblock {\em The Journal of Machine Learning Research}, 18(1):6869--6898,
  2017.

\bibitem{rastegari2016xnor}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In {\em European conference on computer vision}, pages 525--542.
  Springer, 2016.

\bibitem{lecun1989optimal}
Yann LeCun, John Denker, and Sara Solla.
\newblock Optimal brain damage.
\newblock {\em Advances in neural information processing systems}, 2:598--605,
  1989.

\bibitem{hassibi1992second}
Babak Hassibi and David Stork.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock {\em Advances in neural information processing systems}, 5:164--171,
  1992.

\bibitem{molchanov2016pruning}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock {\em arXiv preprint arXiv:1611.06440}, 2016.

\bibitem{li2016pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock {\em arXiv preprint arXiv:1608.08710}, 2016.

\bibitem{liu2018rethinking}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock {\em arXiv preprint arXiv:1810.05270}, 2018.

\bibitem{chippa2013analysis}
Vinay~K Chippa, Srimat~T Chakradhar, Kaushik Roy, and Anand Raghunathan.
\newblock Analysis and characterization of inherent application resilience for
  approximate computing.
\newblock In {\em Proceedings of the 50th Annual Design Automation Conference},
  pages 1--9, 2013.

\bibitem{gillani2020exploiting}
Syed Ghayoor~Abbas Gillani.
\newblock Exploiting error resilience for hardware efficiency: targeting
  iterative and accumulation based algorithms.
\newblock 2020.

\bibitem{zhang2015approxann}
Qian Zhang, Ting Wang, Ye~Tian, Feng Yuan, and Qiang Xu.
\newblock Approxann: An approximate computing framework for artificial neural
  network.
\newblock In {\em 2015 Design, Automation \& Test in Europe Conference \&
  Exhibition (DATE)}, pages 701--706. IEEE, 2015.

\bibitem{carter2010design}
Nicholas~P Carter, Helia Naeimi, and Donald~S Gardner.
\newblock Design techniques for cross-layer resilience.
\newblock In {\em 2010 Design, Automation \& Test in Europe Conference \&
  Exhibition (DATE 2010)}, pages 1023--1028. IEEE, 2010.

\bibitem{lotrivc2012applicability}
Uro{\v{s}} Lotri{\v{c}} and Patricio Buli{\'c}.
\newblock Applicability of approximate multipliers in hardware neural networks.
\newblock {\em Neurocomputing}, 96:57--65, 2012.

\bibitem{du2014leveraging}
Zidong Du, Krishna Palem, Avinash Lingamneni, Olivier Temam, Yunji Chen, and
  Chengyong Wu.
\newblock Leveraging the error resilience of machine-learning applications for
  designing highly energy efficient accelerators.
\newblock In {\em 2014 19th Asia and South Pacific design automation conference
  (ASP-DAC)}, pages 201--206. IEEE, 2014.

\bibitem{mrazek2016design}
Vojtech Mrazek, Syed~Shakib Sarwar, Lukas Sekanina, Zdenek Vasicek, and Kaushik
  Roy.
\newblock Design of power-efficient approximate multipliers for approximate
  artificial neural networks.
\newblock In {\em Proceedings of the 35th International Conference on
  Computer-Aided Design}, pages 1--7, 2016.

\bibitem{sarwar2016multiplier}
Syed~Shakib Sarwar, Swagath Venkataramani, Anand Raghunathan, and Kaushik Roy.
\newblock Multiplier-less artificial neurons exploiting error resiliency for
  energy-efficient neural computing.
\newblock In {\em 2016 Design, Automation \& Test in Europe Conference \&
  Exhibition (DATE)}, pages 145--150. IEEE, 2016.

\bibitem{zervakis2021approximate}
Georgios Zervakis, Hassaan Saadat, Hussam Amrouch, Andreas Gerstlauer, Sri
  Parameswaran, and J{\"o}rg Henkel.
\newblock Approximate computing for ml: State-of-the-art, challenges and
  visions.
\newblock In {\em Proceedings of the 26th Asia and South Pacific Design
  Automation Conference}, pages 189--196, 2021.

\bibitem{venkataramani2016efficient}
Swagath Venkataramani, Kaushik Roy, and Anand Raghunathan.
\newblock Efficient embedded learning for iot devices.
\newblock In {\em 2016 21st Asia and South Pacific Design Automation Conference
  (ASP-DAC)}, pages 308--311. IEEE, 2016.

\bibitem{jouppi2017datacenter}
Norman~P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,
  Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al~Borchers, et~al.
\newblock In-datacenter performance analysis of a tensor processing unit.
\newblock In {\em Proceedings of the 44th annual international symposium on
  computer architecture}, pages 1--12, 2017.

\bibitem{amrouch2020npu}
Hussam Amrouch, Georgios Zervakis, Sami Salamin, Hammam Kattan, Iraklis
  Anagnostopoulos, and J{\"o}rg Henkel.
\newblock Npu thermal management.
\newblock {\em IEEE Transactions on Computer-Aided Design of Integrated
  Circuits and Systems}, 39(11):3842--3855, 2020.

\bibitem{lin2015neural}
Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio.
\newblock Neural networks with few multiplications.
\newblock {\em arXiv preprint arXiv:1510.03009}, 2015.

\bibitem{colangelo2018exploration}
Philip Colangelo, Nasibeh Nasiri, Eriko Nurvitadhi, Asit Mishra, Martin
  Margala, and Kevin Nealis.
\newblock Exploration of low numeric precision deep learning inference using
  intel{\textregistered} fpgas.
\newblock In {\em 2018 IEEE 26th annual international symposium on
  field-programmable custom computing machines (FCCM)}, pages 73--80. IEEE,
  2018.

\bibitem{faraone2019addnet}
Julian Faraone, Martin Kumm, Martin Hardieck, Peter Zipf, Xueyuan Liu, David
  Boland, and Philip~HW Leong.
\newblock Addnet: Deep neural networks using fpga-optimized multipliers.
\newblock {\em IEEE Transactions on Very Large Scale Integration (VLSI)
  Systems}, 28(1):115--128, 2019.

\bibitem{lai2017deep}
Liangzhen Lai, Naveen Suda, and Vikas Chandra.
\newblock Deep convolutional neural network inference with floating-point
  weights and fixed-point activations.
\newblock {\em arXiv preprint arXiv:1703.03073}, 2017.

\bibitem{mcdonnell2011benefits}
Mark~D McDonnell and Lawrence~M Ward.
\newblock The benefits of noise in neural systems: bridging theory and
  experiment.
\newblock {\em Nature Reviews Neuroscience}, 12(7):415--425, 2011.

\bibitem{ernst2007efficient}
Udo Ernst, David Rotermund, and Klaus Pawelzik.
\newblock Efficient computation based on stochastic spikes.
\newblock {\em Neural computation}, 19(5):1313--1343, 2007.

\bibitem{Dapello2020.06.16.154542}
Joel Dapello, Tiago Marques, Martin Schrimpf, Franziska Geiger, David~D. Cox,
  and James~J. DiCarlo.
\newblock Simulating a primary visual cortex at the front of cnns improves
  robustness to image perturbations.
\newblock {\em bioRxiv}, 2020.

\bibitem{rotermund2019Backpropagation}
David Rotermund and Klaus~R. Pawelzik.
\newblock Back-propagation learning in deep spike-by-spike networks.
\newblock {\em Frontiers in Computational Neuroscience}, 13:55, 2019.

\bibitem{rotermund2018massively}
David Rotermund and Klaus~R. Pawelzik.
\newblock Massively parallel {FPGA} hardware for spike-by-spike networks.
\newblock {\em bioRxiv}, 2019.

\bibitem{rotermund2019recurrentsbs}
David Rotermund and Klaus~R. Pawelzik.
\newblock Biologically plausible learning in a deep recurrent spiking network.
\newblock {\em bioRxiv}, 2019.

\bibitem{dayan2001theoretical}
Peter Dayan and Laurence~F Abbott.
\newblock {\em Theoretical neuroscience: computational and mathematical
  modeling of neural systems}.
\newblock Computational Neuroscience Series, 2001.

\bibitem{nevarez2020accelerator}
Yarib Nevarez, Alberto Garcia-Ortiz, David Rotermund, and Klaus~R Pawelzik.
\newblock Accelerator framework of spike-by-spike neural networks for inference
  and incremental learning in embedded systems.
\newblock In {\em 2020 9th International Conference on Modern Circuits and
  Systems Technologies (MOCAST)}, pages 1--5. IEEE, 2020.

\bibitem{li2019sensor}
Guoqiang Li, Chao Deng, Jun Wu, Xuebing Xu, Xinyu Shao, and Yuanhang Wang.
\newblock Sensor data-driven bearing fault diagnosis based on deep
  convolutional neural networks and s-transform.
\newblock {\em Sensors}, 19(12):2750, 2019.

\bibitem{dong2018rolling}
Fei Dong, Xiao Yu, Enjie Ding, Shoupeng Wu, Chunyang Fan, and Yanqiu Huang.
\newblock Rolling bearing fault diagnosis using modified neighborhood
  preserving embedding and maximal overlap discrete wavelet packet transform
  with sensitive features selection.
\newblock {\em Shock and Vibration}, 2018, 2018.

\bibitem{nagayama2007structural}
Tomonori Nagayama and Billie~F Spencer~Jr.
\newblock Structural health monitoring using smart sensors.
\newblock Technical report, Newmark Structural Engineering Laboratory.
  University of Illinois at Urban, 2007.

\bibitem{wang2019deep}
Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng, and Lisha Hu.
\newblock Deep learning for sensor-based activity recognition: A survey.
\newblock {\em Pattern Recognition Letters}, 119:3--11, 2019.

\bibitem{kim2017hazardous}
Yong~Chan Kim, Hyeong-Geun Yu, Jae-Hoon Lee, Dong-Jo Park, and Hyun-Woo Nam.
\newblock Hazardous gas detection for ftir-based hyperspectral imaging system
  using dnn and cnn.
\newblock In {\em Electro-Optical and Infrared Systems: Technology and
  Applications XIV}, volume 10433, page 1043317. International Society for
  Optics and Photonics, 2017.

\bibitem{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484, 2016.

\bibitem{gulshan2016development}
Varun Gulshan, Lily Peng, Marc Coram, Martin~C Stumpe, Derek Wu, Arunachalam
  Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge
  Cuadros, et~al.
\newblock Development and validation of a deep learning algorithm for detection
  of diabetic retinopathy in retinal fundus photographs.
\newblock {\em jama}, 316(22):2402--2410, 2016.

\bibitem{lake2015human}
Brenden~M Lake, Ruslan Salakhutdinov, and Joshua~B Tenenbaum.
\newblock Human-level concept learning through probabilistic program induction.
\newblock {\em Science}, 350(6266):1332--1338, 2015.

\bibitem{xiong2016achieving}
Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas
  Stolcke, Dong Yu, and Geoffrey Zweig.
\newblock Achieving human parity in conversational speech recognition.
\newblock {\em arXiv preprint arXiv:1610.05256}, 2016.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{smetters1996synaptic}
DK~Smetters and Anthony Zador.
\newblock Synaptic transmission: noisy synapses and noisy neurons.
\newblock {\em Current Biology}, 6(10):1217--1218, 1996.

\bibitem{davies2018loihi}
Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao,
  Sri~Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain,
  et~al.
\newblock Loihi: A neuromorphic manycore processor with on-chip learning.
\newblock {\em IEEE Micro}, 38(1):82--99, 2018.

\bibitem{Spinnaker_TransSolid_13}
E.~{Painkras}, L.~A. {Plana}, J.~{Garside}, S.~{Temple}, F.~{Galluppi},
  C.~{Patterson}, D.~R. {Lester}, A.~D. {Brown}, and S.~B. {Furber}.
\newblock Spinnaker: A 1-w 18-core system-on-chip for massively-parallel neural
  network simulation.
\newblock {\em IEEE Journal of Solid-State Circuits}, 48(8):1943--1953, 2013.

\bibitem{roy2019towards}
Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda.
\newblock Towards spike-based machine intelligence with neuromorphic computing.
\newblock {\em Nature}, 575(7784):607--617, 2019.

\bibitem{young2019review}
Aaron~R Young, Mark~E Dean, James~S Plank, and Garrett~S Rose.
\newblock A review of spiking neuromorphic hardware communication systems.
\newblock {\em IEEE Access}, 7:135606--135620, 2019.

\bibitem{TrueNorth_Trans15}
F.~{Akopyan}, J.~{Sawada}, A.~{Cassidy}, R.~{Alvarez-Icaza}, J.~{Arthur},
  P.~{Merolla}, N.~{Imam}, Y.~{Nakamura}, P.~{Datta}, G.~{Nam}, B.~{Taba},
  M.~{Beakes}, B.~{Brezzo}, J.~B. {Kuang}, R.~{Manohar}, W.~P. {Risk},
  B.~{Jackson}, and D.~S. {Modha}.
\newblock Truenorth: Design and tool flow of a 65 mw 1 million neuron
  programmable neurosynaptic chip.
\newblock {\em IEEE Trans. on Computer-Aided Design of Integrated Circuits and
  Systems}, 34(10):1537--1557, Oct 2015.

\bibitem{izhikevich2004model}
Eugene~M Izhikevich.
\newblock Which model to use for cortical spiking neurons?
\newblock {\em IEEE transactions on neural networks}, 15(5):1063--1070, 2004.

\bibitem{amunts2019human}
Katrin Amunts, Alois~C Knoll, Thomas Lippert, Cyriel~MA Pennartz, Philippe
  Ryvlin, Alain Destexhe, Viktor~K Jirsa, Egidio D?Angelo, and Jan~G Bjaalie.
\newblock The human brain project -- synergy between neuroscience, computing,
  informatics, and brain-inspired technologies.
\newblock {\em PLoS biology}, 17(7):e3000344, 2019.

\bibitem{gerstner2014neuronal}
Wulfram Gerstner, Werner~M Kistler, Richard Naud, and Liam Paninski.
\newblock {\em Neuronal dynamics: From single neurons to networks and models of
  cognition}.
\newblock Cambridge University Press, 2014.

\bibitem{merolla2014million}
Paul~A Merolla, John~V Arthur, Rodrigo Alvarez-Icaza, Andrew~S Cassidy, Jun
  Sawada, Filipp Akopyan, Bryan~L Jackson, Nabil Imam, Chen Guo, Yutaka
  Nakamura, et~al.
\newblock A million spiking-neuron integrated circuit with a scalable
  communication network and interface.
\newblock {\em Science}, 345(6197):668--673, 2014.

\bibitem{neftci2019surrogate}
Emre~O Neftci, Hesham Mostafa, and Friedemann Zenke.
\newblock Surrogate gradient learning in spiking neural networks: Bringing the
  power of gradient-based optimization to spiking neural networks.
\newblock {\em IEEE Signal Processing Magazine}, 36(6):51--63, 2019.

\bibitem{lee2016training}
Jun~Haeng Lee, Tobi Delbruck, and Michael Pfeiffer.
\newblock Training deep spiking neural networks using backpropagation.
\newblock {\em Frontiers in neuroscience}, 10:508, 2016.

\bibitem{masquelier2007unsupervised}
Timoth{\'e}e Masquelier and Simon~J Thorpe.
\newblock Unsupervised learning of visual features through spike timing
  dependent plasticity.
\newblock {\em PLoS computational biology}, 3(2):e31, 2007.

\bibitem{rotermund2019back}
David Rotermund and Klaus~R. Pawelzik.
\newblock Back-propagation learning in deep spike-by-spike networks.
\newblock {\em Frontiers in Computational Neuroscience}, 13:55, 2019.

\bibitem{shadlen1994noise}
Michael~N Shadlen and William~T Newsome.
\newblock Noise, neural codes and cortical organization.
\newblock {\em Current opinion in neurobiology}, 4(4):569--579, 1994.

\bibitem{softky1993highly}
William~R Softky and Christof Koch.
\newblock The highly irregular firing of cortical cells is inconsistent with
  temporal integration of random epsps.
\newblock {\em Journal of Neuroscience}, 13(1):334--350, 1993.

\bibitem{ganguli2012compressed}
Surya Ganguli and Haim Sompolinsky.
\newblock Compressed sensing, sparsity, and dimensionality in neuronal
  information processing and data analysis.
\newblock {\em Annual review of neuroscience}, 35:485--508, 2012.

\bibitem{ganguli2010statistical}
Surya Ganguli and Haim Sompolinsky.
\newblock Statistical mechanics of compressed sensing.
\newblock {\em Physical review letters}, 104(18):188701, 2010.

\bibitem{pfeiffer2018deep}
Michael Pfeiffer and Thomas Pfeil.
\newblock Deep learning with spiking neurons: Opportunities and challenges.
\newblock {\em Frontiers in neuroscience}, 12, 2018.

\bibitem{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock {\em http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem{rosenblatt1958perceptron}
Frank Rosenblatt.
\newblock The perceptron: a probabilistic model for information storage and
  organization in the brain.
\newblock {\em Psychological review}, 65(6):386, 1958.

\bibitem{rumelhart1986learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em nature}, 323(6088):533, 1986.

\bibitem{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem{maas2013rectifier}
Andrew~L Maas, Awni~Y Hannun, Andrew~Y Ng, et~al.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In {\em Proc. icml}, volume~30, page~3. Atlanta, GA, 2013.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem{bottou2010large}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In {\em Proceedings of COMPSTAT'2010: 19th International Conference
  on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited
  and Contributed Papers}, pages 177--186. Springer, 2010.

\bibitem{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em SIAM review}, 60(2):223--311, 2018.

\bibitem{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em International conference on machine learning}, pages
  1139--1147. PMLR, 2013.

\bibitem{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of machine learning research}, 12(7), 2011.

\bibitem{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock {\em Deep learning}.
\newblock MIT press, 2016.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{gu2018recent}
Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai,
  Ting Liu, Xingxing Wang, Gang Wang, Jianfei Cai, et~al.
\newblock Recent advances in convolutional neural networks.
\newblock {\em Pattern Recognition}, 77:354--377, 2018.

\bibitem{sze2017efficient}
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel~S Emer.
\newblock Efficient processing of deep neural networks: A tutorial and survey.
\newblock {\em Proceedings of the IEEE}, 105(12):2295--2329, 2017.

\bibitem{micikevicius2017mixed}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
  David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
  Venkatesh, et~al.
\newblock Mixed precision training.
\newblock {\em arXiv preprint arXiv:1710.03740}, 2017.

\bibitem{courbariaux2014training}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Training deep neural networks with low precision multiplications.
\newblock {\em arXiv preprint arXiv:1412.7024}, 2014.

\bibitem{jacob2018quantization}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew
  Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2704--2713, 2018.

\bibitem{zuras2008ieee}
Dan Zuras, Mike Cowlishaw, Alex Aiken, Matthew Applegate, David Bailey, Steve
  Bass, Dileep Bhandarkar, Mahesh Bhat, David Bindel, Sylvie Boldo, et~al.
\newblock Ieee standard for floating-point arithmetic.
\newblock {\em IEEE Std}, 754(2008):1--70, 2008.

\bibitem{krishnamoorthi2018quantizing}
Raghuraman Krishnamoorthi.
\newblock Quantizing deep convolutional networks for efficient inference: A
  whitepaper.
\newblock {\em arXiv preprint arXiv:1806.08342}, 2018.

\bibitem{moons20160}
Bert Moons and Marian Verhelst.
\newblock A 0.3--2.6 tops/w precision-scalable processor for real-time
  large-scale convnets.
\newblock In {\em 2016 IEEE Symposium on VLSI Circuits (VLSI-Circuits)}, pages
  1--2. IEEE, 2016.

\bibitem{whatmough201714}
Paul~N Whatmough, Sae~Kyu Lee, Hyunkwang Lee, Saketh Rama, David Brooks, and
  Gu-Yeon Wei.
\newblock 14.3 a 28nm soc with a 1.2 ghz 568nj/prediction sparse
  deep-neural-network engine with> 0.1 timing error rate tolerance for iot
  applications.
\newblock In {\em 2017 IEEE International Solid-State Circuits Conference
  (ISSCC)}, pages 242--243. IEEE, 2017.

\bibitem{sun2018xnor}
Xiaoyu Sun, Shihui Yin, Xiaochen Peng, Rui Liu, Jae-sun Seo, and Shimeng Yu.
\newblock Xnor-rram: A scalable and parallel resistive synaptic architecture
  for binary neural networks.
\newblock In {\em 2018 Design, Automation \& Test in Europe Conference \&
  Exhibition (DATE)}, pages 1423--1428. IEEE, 2018.

\bibitem{rathi2018stdp}
Nitin Rathi, Priyadarshini Panda, and Kaushik Roy.
\newblock Stdp-based pruning of connections and weight quantization in spiking
  neural networks for energy-efficient recognition.
\newblock {\em IEEE Transactions on Computer-Aided Design of Integrated
  Circuits and Systems}, 38(4):668--677, 2018.

\bibitem{sen2017approximate}
Sanchari Sen, Swagath Venkataramani, and Anand Raghunathan.
\newblock Approximate computing for spiking neural networks.
\newblock In {\em Design, Automation \& Test in Europe Conference \& Exhibition
  (DATE), 2017}, pages 193--198. IEEE, 2017.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem{wan2013regularization}
Li~Wan, Matthew Zeiler, Sixin Zhang, Yann Le~Cun, and Rob Fergus.
\newblock Regularization of neural networks using dropconnect.
\newblock In {\em International conference on machine learning}, pages
  1058--1066, 2013.

\bibitem{neftci2016stochastic}
Emre~O Neftci, Bruno~U Pedroni, Siddharth Joshi, Maruan Al-Shedivat, and Gert
  Cauwenberghs.
\newblock Stochastic synapses enable efficient brain-inspired learning
  machines.
\newblock {\em Frontiers in neuroscience}, 10:241, 2016.

\bibitem{srinivasan2016magnetic}
Gopalakrishnan Srinivasan, Abhronil Sengupta, and Kaushik Roy.
\newblock Magnetic tunnel junction based long-term short-term stochastic
  synapse for a spiking neural network with on-chip stdp learning.
\newblock {\em Scientific reports}, 6:29545, 2016.

\bibitem{buesing2011neural}
Lars Buesing, Johannes Bill, Bernhard Nessler, and Wolfgang Maass.
\newblock Neural dynamics as sampling: a model for stochastic computation in
  recurrent networks of spiking neurons.
\newblock {\em PLoS Comput Biol}, 7(11):e1002211, 2011.

\bibitem{bellec2017deep}
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock {\em arXiv preprint arXiv:1711.05136}, 2017.

\bibitem{chen20184096}
Gregory~K Chen, Raghavan Kumar, H~Ekin Sumbul, Phil~C Knag, and Ram~K
  Krishnamurthy.
\newblock A 4096-neuron 1m-synapse 3.8-pj/sop spiking neural network with
  on-chip stdp learning and sparse weights in 10-nm finfet cmos.
\newblock {\em IEEE Journal of Solid-State Circuits}, 54(4):992--1002, 2018.

\bibitem{sheik2016synaptic}
Sadique Sheik, Somnath Paul, Charles Augustine, Chinnikrishna Kothapalli,
  Muhammad~M Khellah, Gert Cauwenberghs, and Emre Neftci.
\newblock Synaptic sampling in hardware spiking neural networks.
\newblock In {\em 2016 IEEE International Symposium on Circuits and Systems
  (ISCAS)}, pages 2090--2093. IEEE, 2016.

\bibitem{jerry2017ultra}
M~Jerry, A~Parihar, B~Grisafe, A~Raychowdhury, and S~Datta.
\newblock Ultra-low power probabilistic imt neurons for stochastic sampling
  machines.
\newblock In {\em 2017 Symposium on VLSI Circuits}, pages T186--T187. IEEE,
  2017.

\bibitem{zhang2018survey}
Ming ZHANG, GU~Zonghua, and PAN Gang.
\newblock A survey of neuromorphic computing based on spiking neural networks.
\newblock {\em Chinese Journal of Electronics}, 27(4):667--674, 2018.

\bibitem{kim2013energy}
Yongtae Kim, Yong Zhang, and Peng Li.
\newblock An energy efficient approximate adder with carry skip for error
  resilient neuromorphic vlsi systems.
\newblock In {\em 2013 IEEE/ACM International Conference on Computer-Aided
  Design (ICCAD)}, pages 130--137. IEEE, 2013.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 115(3):211--252,
  2015.

\bibitem{lian2019high}
Xiaocong Lian, Zhenyu Liu, Zhourui Song, Jiwu Dai, Wei Zhou, and Xiangyang Ji.
\newblock High-performance fpga-based cnn accelerator with block-floating-point
  arithmetic.
\newblock {\em IEEE Transactions on Very Large Scale Integration (VLSI)
  Systems}, 27(8):1874--1885, 2019.

\bibitem{mei2017200mhz}
Chunsheng Mei, Zhenyu Liu, Yue Niu, Xiangyang Ji, Wei Zhou, and Dongsheng Wang.
\newblock A 200mhz 202.4 gflops@ 10.8 w vgg16 accelerator in xilinx vx690t.
\newblock In {\em 2017 IEEE Global Conference on Signal and Information
  Processing (GlobalSIP)}, pages 784--788. IEEE, 2017.

\bibitem{wu2021low}
Chen Wu, Mingyu Wang, Xinyuan Chu, Kun Wang, and Lei He.
\newblock Low-precision floating-point arithmetic for high-performance
  fpga-based cnn acceleration.
\newblock {\em ACM Transactions on Reconfigurable Technology and Systems
  (TRETS)}, 15(1):1--21, 2021.

\bibitem{meloni2019cnn}
Paolo Meloni, Antonio Garufi, Gianfranco Deriu, Marco Carreras, and Daniela
  Loi.
\newblock Cnn hardware acceleration on a low-power and low-cost apsoc.
\newblock In {\em 2019 Conference on Design and Architectures for Signal and
  Image Processing (DASIP)}, pages 7--12. IEEE, 2019.

\bibitem{wu2020phoenix}
Chen Wu, Mingyu Wang, Xiayu Li, Jicheng Lu, Kun Wang, and Lei He.
\newblock Phoenix: A low-precision floating-point quantization oriented
  architecture for convolutional neural networks.
\newblock {\em arXiv preprint arXiv:2003.02628}, 2020.

\bibitem{park2021neural}
Jeongwoo Park, Sunwoo Lee, and Dongsuk Jeon.
\newblock A neural network training processor with 8-bit shared exponent bias
  floating point and multiple-way fused multiply-add trees.
\newblock {\em IEEE Journal of Solid-State Circuits}, 57(3):965--977, 2021.

\bibitem{venkataramani2021rapid}
Swagath Venkataramani, Vijayalakshmi Srinivasan, Wei Wang, Sanchari Sen, Jintao
  Zhang, Ankur Agrawal, Monodeep Kar, Shubham Jain, Alberto Mannari, Hoang
  Tran, et~al.
\newblock Rapid: Ai accelerator for ultra-low precision training and inference.
\newblock In {\em 2021 ACM/IEEE 48th Annual International Symposium on Computer
  Architecture (ISCA)}, pages 153--166. IEEE, 2021.

\bibitem{venkataramanaiah202228nm}
Shreyas~Kolala Venkataramanaiah, Jian Meng, Han-Sok Suh, Injune Yeo, Jyotishman
  Saikia, Sai~Kiran Cherupally, Yichi Zhang, Zhiru Zhang, and Jae-sun Seo.
\newblock A 28nm 8-bit floating-point tensor core based cnn training processor
  with dynamic activation/weight sparsification.
\newblock In {\em ESSCIRC 2022-IEEE 48th European Solid State Circuits
  Conference (ESSCIRC)}, pages 89--92. IEEE, 2022.

\bibitem{gallus2018handwritten}
Michal Gallus and Alberto Nannarelli.
\newblock Handwritten digit classification using 8-bit floating point based
  convolutional neural networks.
\newblock Technical report, DTU Compute Technical Report, 2018.

\bibitem{wang2018training}
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash
  Gopalakrishnan.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{sun2019hybrid}
Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani,
  Vijayalakshmi~Viji Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash
  Gopalakrishnan.
\newblock Hybrid 8-bit floating point (hfp8) training and inference for deep
  neural networks.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{mellempudi2019mixed}
Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul.
\newblock Mixed precision training with 8-bit floating point.
\newblock {\em arXiv preprint arXiv:1905.12334}, 2019.

\bibitem{liu2021improving}
Fangxin Liu, Wenbo Zhao, Zhezhi He, Yanzhi Wang, Zongwu Wang, Changzhi Dai,
  Xiaoyao Liang, and Li~Jiang.
\newblock Improving neural network efficiency via post-training quantization
  with adaptive floating-point.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 5281--5290, 2021.

\bibitem{schmidhuber2015deep}
J{\"u}rgen Schmidhuber.
\newblock Deep learning in neural networks: An overview.
\newblock {\em Neural networks}, 61:85--117, 2015.

\bibitem{Taigman_2014_CVPR}
Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, and Lior Wolf.
\newblock Deepface: Closing the gap to human-level performance in face
  verification.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2014.

\bibitem{Design_Exploration_SbS_Trans20}
Nassim Abderrahmane, Edgar Lemaire, and Benoît Miramond.
\newblock Design space exploration of hardware spiking neurons for embedded
  artificial intelligence.
\newblock {\em Neural Networks}, 121:366 -- 386, 2020.

\bibitem{Spinnaker_Trans13}
E.~{Painkras}, L.~A. {Plana}, J.~{Garside}, S.~{Temple}, F.~{Galluppi},
  C.~{Patterson}, D.~R. {Lester}, A.~D. {Brown}, and S.~B. {Furber}.
\newblock Spinnaker: A 1-w 18-core system-on-chip for massively-parallel neural
  network simulation.
\newblock {\em IEEE Journal of Solid-State Circuits}, 48(8):1943--1953, Aug
  2013.

\bibitem{SNN_Survey_Trans19}
Maxence Bouvier, Alexandre Valentian, Thomas Mesquida, Francois Rummens, Marina
  Reyboz, Elisa Vianello, and Edith Beigne.
\newblock Spiking neural networks hardware implementations and challenges: A
  survey.
\newblock {\em J. Emerg. Technol. Comput. Syst.}, 15(2), April 2019.

\bibitem{park2009dynamic}
Jongsun Park, Jung~Hwan Choi, and Kaushik Roy.
\newblock Dynamic bit-width adaptation in dct: An approach to trade off image
  quality and computation energy.
\newblock {\em IEEE transactions on very large scale integration (VLSI)
  systems}, 18(5):787--793, 2009.

\bibitem{gupta2011impact}
Vaibhav Gupta, Debabrata Mohapatra, Sang~Phill Park, Anand Raghunathan, and
  Kaushik Roy.
\newblock Impact: imprecise adders for low-power approximate computing.
\newblock In {\em IEEE/ACM International Symposium on Low Power Electronics and
  Design}, pages 409--414. IEEE, 2011.

\bibitem{mittal2016survey}
Sparsh Mittal.
\newblock A survey of techniques for approximate computing.
\newblock {\em ACM Computing Surveys (CSUR)}, 48(4):1--33, 2016.

\bibitem{venkataramani2015approximate}
Swagath Venkataramani, Srimat~T Chakradhar, Kaushik Roy, and Anand Raghunathan.
\newblock Approximate computing and the quest for computing efficiency.
\newblock In {\em 2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC)},
  pages 1--6. IEEE, 2015.

\bibitem{xilinx2015zynq}
UG585 Xilinx.
\newblock Zynq-7000 all programmable soc: Technical reference manual, 2015.

\bibitem{hrica2012floating}
James Hrica.
\newblock Floating-point design with vivado hls.
\newblock {\em Xilinx Application Note}, 2012.

\bibitem{lom2016industry}
Michal Lom, Ondrej Pribyl, and Miroslav Svitek.
\newblock Industry 4.0 as a part of smart cities.
\newblock In {\em 2016 Smart Cities Symposium Prague (SCSP)}, pages 1--6. IEEE,
  2016.

\bibitem{ince2016real}
Turker Ince, Serkan Kiranyaz, Levent Eren, Murat Askar, and Moncef Gabbouj.
\newblock Real-time motor fault detection by 1-d convolutional neural networks.
\newblock {\em IEEE Transactions on Industrial Electronics}, 63(11):7067--7075,
  2016.

\bibitem{janssens2016convolutional}
Olivier Janssens, Viktor Slavkovikj, Bram Vervisch, Kurt Stockman, Mia
  Loccufier, Steven Verstockt, Rik Van~de Walle, and Sofie Van~Hoecke.
\newblock Convolutional neural network based fault detection for rotating
  machinery.
\newblock {\em Journal of Sound and Vibration}, 377:331--345, 2016.

\bibitem{abdeljaber2017real}
Osama Abdeljaber, Onur Avci, Serkan Kiranyaz, Moncef Gabbouj, and Daniel~J
  Inman.
\newblock Real-time vibration-based structural damage detection using
  one-dimensional convolutional neural networks.
\newblock {\em Journal of Sound and Vibration}, 388:154--170, 2017.

\bibitem{guo2016hierarchical}
Xiaojie Guo, Liang Chen, and Changqing Shen.
\newblock Hierarchical adaptive deep convolution neural network and its
  application to bearing fault diagnosis.
\newblock {\em Measurement}, 93:490--502, 2016.

\bibitem{nurvitadhi2017can}
Eriko Nurvitadhi, Ganesh Venkatesh, Jaewoong Sim, Debbie Marr, Randy Huang,
  Jason Ong Gee~Hock, Yeong~Tat Liew, Krishnan Srivatsan, Duncan Moss, Suchit
  Subhaschandra, et~al.
\newblock Can fpgas beat gpus in accelerating next-generation deep neural
  networks?
\newblock In {\em Proceedings of the 2017 ACM/SIGDA International Symposium on
  Field-Programmable Gate Arrays}, pages 5--14, 2017.

\bibitem{han2015learning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{nevarez2021accelerating}
Yarib Nevarez, David Rotermund, Klaus~R Pawelzik, and Alberto Garcia-Ortiz.
\newblock Accelerating spike-by-spike neural networks on fpga with hybrid
  custom floating-point and logarithmic dot-product approximation.
\newblock {\em IEEE Access}, 2021.

\bibitem{hannwindowsine}
Shirsendu Sikdar, Sauvik Banerjee, and G.~Ashish.
\newblock Ultrasonic guided wave propagation and disbond identification in a
  honeycomb composite sandwich structure using bonded piezoelectric wafer
  transducers.
\newblock {\em Journal of Intelligent Material Systems and Structures}, 27, 10
  2015.

\bibitem{stft_lit}
U.~Kiencke, M.~Schwarz, and T.~Weickert.
\newblock {\em Signalverarbeitung: Zeit-Frequenz-Analysen und
  Schätzverfahren}.
\newblock Oldenbourh, 2008.

\bibitem{blackman_window}
R.~B. Blackman and J.~W. Tukey.
\newblock The measurement of power spectra from the point of view of
  communications engineering - part i.
\newblock {\em Bell System Technical Journal}, 37(1):185--282, 1958.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{springenberg2014striving}
Jost~Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin
  Riedmiller.
\newblock Striving for simplicity: The all convolutional net.
\newblock {\em arXiv preprint arXiv:1412.6806}, 2014.

\end{thebibliography}
