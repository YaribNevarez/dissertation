\chapter{Conclusion and Outlook}
\label{chap.conclusion}
\minitoc
The field of artificial intelligence is launching an era characterized by the prevalence of ubiquitous connected devices. To ensure the sustainability of this transformation, it is crucial to harmonize computational accuracy with energy efficiency and the compatibility of hardware solutions. This dissertation focuses on enhancing the efficiency of AI hardware engines, taking into account these considerations.

\section{State-of-the-art challenges and solutions}
Widely used \gls{ai}/\gls{ml} algorithms, \glspl{snn} and \glspl{cnn}, come with elevated computational and energy demands. The intrinsic error resilience of these algorithms brings "approximate computing" paradigms, such as quantization, to the forefront, offering efficiency enhancements. This research presents cutting-edge methodologies centered on low-power neural network accelerator designs employing custom \gls{fp} computation.

\section{Key Contributions}
\begin{itemize}
	\item \textbf{Hybrid 8-bit Floating-Point and 4-bit Logarithmic Computation}: A hardware design methodology is presented for low-power \gls{sbs} neural networks targeting embedded applications. This approach leverages the intrinsic error resilience of \gls{sbs}, emphasizing a balance between performance and hardware complexity. Significant reductions in run-time, memory footprint, and power consumption are realized, with minimal accuracy trade-offs.
	
	\item \textbf{Hybrid 6-bit Floating-Point Computation}: A novel low-power hardware design technique tailored for resource-constrained applications is presented. The \gls{hf6} quantization strategy, its specialized hardware \gls{mac} unit, and the tensor processor are showcased. Compatibility with TensorFlow Lite demonstrates its industry relevance and potential for broader adoption.
\end{itemize}

\section{Future Directions}

Future directions focus on optimizing energy efficiency and computational performance for on-device learning techniques. Key strategies include reduced gradient formats for optimized data processing, and exploring reduced hardware architectures with lower-bit formats like Bfloat16 in feature maps. This can significantly optimize memory usage and reduce energy consumption while maintaining \gls{qor}.

Additionally, in this work, there is potential to boost computational throughput by shifting from pipeline to parallel computation structures, augmented by wider memory channels. This approach not only enhances processing speed but also broadens the application scope beyond sensor analytics to a comprehensive range of \gls{ai}/\gls{ml} models for both inference and learning. The goal is to leverage hybrid reduced \gls{fp} quantization, aligning with the need for more energy-efficient on-device learning in various \gls{ai}/\gls{ml} applications.


\section{Final Remarks}
This dissertation delves into design techniques that exploit the intrinsic error resilience of \gls{ai}/\gls{ml} algorithms, focusing on optimal \gls{fp} inference acceleration in resource-constrained embedded systems. Key takeaways include:

\begin{enumerate}
	\item Quantization techniques, particularly those involving reduced floating-point formats, are set to significantly improve hardware designs. These enhancements include acceleration of computation, increased energy efficiency, and optimized chip area. Additionally, they positively impact the versatility, portability, and compatibility aspects of hardware solutions.
	
	\item The \gls{mac} module designs presented in this work showcase a balance between computational accuracy and resource efficiency, suitable for resource-constrained embedded devices. This approach has relevance and applicability in both academic research and industrial contexts.
	
	\item The \gls{hf6} quantization approach has the potential to be applicable across the entire spectrum of \gls{ai}/\gls{ml} models. This method enhances energy efficiency, processing speed, and memory footprint, particularly important in the real-world application of \gls{ai}/\gls{ml} technologies.
	
	\item The methodologies showcased for low-power inference acceleration hold significant potential for adaptation to the high computational demands of data centers. Training \gls{ai}/\gls{ml} models using reduced-precision floating-point hardware architectures achieves effects akin to those of \gls{qat} methods. Implementing the proposed techniques in data centers could lead to substantial improvements in energy and resource efficiency.
\end{enumerate}

