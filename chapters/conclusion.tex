\chapter{Conclusion and Outlook}
\label{chap.conclusion}
\minitoc
Artificial intelligence is launching an era defined by ubiquitous connected devices. To ensure the sustainability of this transformation, it is essential to merge accurate computational results with economical system designs. This dissertation emphasizes refining the efficiency of \gls{ai} hardware engines and enhancing portability.

\section{State-of-the-art challenges and solutions}
Prominent \gls{ai}\gls{ml} algorithms, notably \glspl{snn} and \glspl{cnn}, come with elevated computational and energy demands. The intrinsic error resilience of these algorithms brings "approximate computing" paradigms, such as quantization, to the forefront, offering vital efficiency enhancements. This research delineates cutting-edge methodologies centered on low-power neural network accelerator designs employing custom \gls{fp} computation.

\section{Specific Contributions}
\begin{itemize}
	\item \textbf{\gls{snn}}: A hardware design methodology is presented for low-power \gls{sbs} neural networks targeting embedded applications. This approach leverages the intrinsic error resilience of \gls{sbs}, emphasizing a balance between performance and hardware complexity. Significant reductions in run-time, memory footprint, and power consumption are realized, with minimal accuracy trade-offs.
	
	\item \textbf{\gls{cnn}}: A novel low-power hardware design technique tailored resource-constrained applications is presented. The \gls{hf6} quantization strategy, its specialized hardware \gls{mac} unit and tensor processor is showcased. Compatibility with TensorFlow Lite demonstrates its industry relevance and potential for broader adoption.
\end{itemize}

\section{Future Directions}
\begin{itemize}
	\item \textbf{Efficiency Improvements}: Exploring reduced architectures using lower-bit formats such as Bfloat16 in feature maps can optimize memory usage and energy consumption.
	
	\item \textbf{Enhanced Throughput}: There is scope in boosting computational throughput by transitioning from pipeline computation to parallel structures, incorporating wider memory channels, and potentially \gls{asic} implementations for greater efficiency.
	
	\item \textbf{Broadened Application Scope}: While the current emphasis is on sensor analytics, future designs will target the comprehensive spectrum of \gls{ai}/\gls{ml} models for inference and learning.
\end{itemize}

\section{Final Remarks}
Given neural networks' inherent flexibility regarding precision, approximate computing offers remarkable improvements in processing efficiency with minor accuracy degradation. This dissertation examines deep into designs leveraging the intrinsic error resilience of \gls{ml} algorithms, aiming for optimal \gls{fp} inference on energy-efficient embedded systems. The proposed methodologies possess the adaptability and resilience for both low-power on-device training and scaling up to the high computational demands of data centers. Key takeaways include:

\begin{enumerate}
	\item Approximate computing techniques, especially quantization, are set to revolutionize hardware designs by optimizing computation acceleration, energy efficiency, and chip area.
	
	\item The proposed \gls{mac} module design offers a perfect blend of computational accuracy and resource efficiency, aligning with the needs of resource-constrained devices.
	
	
	\item The \gls{hf6} quantization approach, harmoniously integrating with \gls{fp} standards and TensorFlow Lite, establishes a foundation not only for diverse applications but also across the complete spectrum of \gls{ai}/\gls{ml} models, highlighting the synergy between innovative solutions and industry significance.
	
	\item Collectively, the methodologies and designs proposed here mark a pivotal step towards the development of a sustainable next-gen neural network processor ecosystem, properly supporting an interconnected world, spanning from individual devices to expansive data centers.
\end{enumerate}

