\chapter*{Abstract}
\thispagestyle{empty}
The use of \gls{ai} is entering a new era based on the use of ubiquitous connected devices. The sustainability of this transformation requires the adoption of design techniques that reconcile accurate results with cost-effective system architectures. As such, improving the efficiency of \gls{ai} hardware engines as well as \gls{ml} portability must be considered.

In the emerging era of Industry 4.0, \gls{ml} algorithms yield the power of \gls{ai} to massively ubiquitous \gls{iot} devices. Applications in this field become smarter and more profitable as the availability of big data gets expanded, driving evolution of many aspects in science, industry, and daily life. However, state-of-the-art \gls{ml} algorithms, specially \glspl{snn} and \glspl{cnn}, represent elevated computational and energy cost. Therefore, hardware efficiency is one of the major goals to innovate compute engines as they are the machinery of the future.

Energy, performance, and chip-area are the key design concerns in computer systems. Considering the intrinsic error resilience of \gls{ml} algorithms, paradigms such as approximate computing come to the rescue by offering promising efficiency gains to assist the aforementioned design concerns. Approximation techniques are widely used in \gls{ml} algorithms at the model-structure as well as at the hardware processing level. However, state-of-the-art methods do not sufficiently address accelerator designs for \glspl{ann}, in particular with \gls{fp} computation.

To sustain the continuous expansion of \gls{ml} applications on cost-effective compute devices, approximate computing has the potential to gradually transform from a design alternative to an essential feature. This dissertation focuses on the investigation of design methodologies to exploit the intrinsic error resilience of \gls{ml} algorithms to optimize high-quality \gls{fp} inference in low-power embedded systems.

In the field of \gls{snn}, this dissertation presents a hardware design methodology for low-power inference of \gls{sbs} neural networks targeting embedded applications. This \gls{ml} algorithm provides exceptional noise robustness and reduced complexity compared to conventional \gls{snn} with \gls{lif} mechanism. However, \gls{sbs} networks represent a memory footprint and a computational cost unsuitable for embedded applications. To address this problem, this research exploits the intrinsic error resilience of \gls{sbs} to improve performance and to reduce hardware complexity. More precisely, it is proposed a vector dot-product module based on approximate computing with configurable quality using hybrid custom \gls{fp} and logarithmic number representations.

In the field of \gls{cnn}, this dissertation presents a hardware design methodology for low-power inference targeting sensor analytics applications. In this work, it is proposed the \gls{hf6} quantization and its dedicated hardware processor. This quantization allows an optimized \gls{fp} \gls{mac} hardware design by reducing the mantissa multiplication to a multiplexer-adder operation. Additionally, this design exploits the intrinsic error tolerance of neural networks to further reduce the hardware architecture with approximation on the \gls{fp} subnormal number computation. For \gls{ml} portability, the custom \gls{fp} representation is wrapped in the standard format, which is automatically extracted by the proposed hardware. The hardware/software architecture is integrated with TensorFlow Lite to demonstrate portability and backward compatibility with industry standard \gls{ml} frameworks.

The outcome of this dissertation aims to contribute to the rise of a sustainable next generation of energy efficient neural network processors with \gls{ml} portability and high-accuracy as design requirements.