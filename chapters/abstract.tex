\chapter*{Abstract}
\thispagestyle{empty}

The expansion of \gls{ai} is addressing a new era characterized by omnipresent, connected devices. To ensure the sustainability of this transformative phase, it is imperative to adopt design strategies that harmonize precise computational results with economically viable system architectures. Consequently, refining the efficiency and quality of \gls{ai} hardware engines along with enhancing the interoperability of \gls{ml} algorithms stand as critical considerations in this evolving landscape. This necessitates a balanced approach that prioritizes energy-efficient computations, precise and reliable results, and seamless integration across various platforms and devices.

In the context of Industry 4.0, \gls{ml} algorithms are serving as the foundational enabler for the widespread integration of \gls{ai} into \gls{iot} devices. These advancements are shaping applications to be more intelligent and economically rewarding, a transformation fueled by the exponential growth of available big data. This data revolution is influencing numerous domains, from scientific research to industrial processes and everyday living. However, this technological evolution comes with its own set of intricate challenges. Contemporary \gls{ml} algorithms, specifically \glspl{snn} and \glspl{cnn}, pose significant computational and energy demands. Consequently, a central objective of this dissertation is to explore innovative methods for enhancing the hardware efficiency and quality of computing engines, which are poised to be the machinery unlocking the future of this rapidly evolving field.

Approximate computing techniques, such as quantization, exploit the inherent error resilience of \gls{ml} algorithms to address key design concerns in computer systems: energy efficiency, performance, and chip area. Quantization, which involves reducing the number of bits used to represent numbers, can significantly lower power consumption and data movement, thereby enhancing energy efficiency, and enables compact arithmetic units that save chip area. These techniques often yield computation speedups due to reduced data sizes, promoting faster, more parallel, and pipelined processing, particularly in specialized hardware accelerators designed for \gls{ann}. However, this approach introduces a trade-off between precision and model accuracy, necessitating proper hardware design methodologies. While state-of-the-art methods are advancing, significant research opportunities remain, especially for developing custom low-power and reduced-precision \gls{fp} computation tailored for \glspl{ann}.

In this dissertation, a hardware design methodology is presented for low-power inference of \gls{sbs} neural networks in the context of embedded applications, within the broader field of \glspl{snn}. Compared to conventional \glspl{snn} employing the \gls{lif} mechanism, \gls{sbs} neural networks are highlighted for their exceptional noise robustness and reduced complexity. However, despite their advantages, \gls{sbs} networks inherently possess a memory footprint and computational cost that makes them challenging for deployment in constrained devices. To mitigate this issue, this research exploits the intrinsic error resilience of \gls{sbs} models, aiming to boost performance and reduce hardware complexity, while avoiding the need for model quantization or retraining. Specifically, this research introduces a novel hardware \gls{mac} module designed to optimize the balance between computational accuracy and resource efficiency for low-power applications. This \gls{mac} module features configurable quality through a unique hybrid approach. It combines standard \gls{fp} number representations with a custom 8-bit \gls{fp} format, as well as a 4-bit logarithmic number representation. Notably, this design excludes the use of a sign bit, further contributing to the compact and efficient representation of numbers. This strategic and flexible design enables the \gls{mac} module to be tailored to the specific resource constraints and performance requirements of a given embedded application, thereby making \gls{sbs} neural networks possible for deployment in low-power, resource-constrained environments.

In the field of \glspl{cnn}, this dissertation presents a hardware design methodology for low-power inference, specifically targeting high-quality sensor analytics applications. Central to this work is the proposal of the \gls{hf6} quantization scheme and its dedicated hardware accelerator, designed to function as a Conv2D \gls{tp}. This quantization strategy employs a hybrid number representation, combining standard \gls{fp} and a lean 6-bit \gls{fp} format. This design choice allows for a highly optimized \gls{fp} \gls{mac} hardware configuration, notably simplifying the mantissa multiplication into an efficient multiplexer-adder operation. Leveraging the inherent error tolerance of neural networks, the design further reduces the hardware architecture through approximations in the \gls{fp} subnormal number computation. This research introduces a \gls{qat} method that, in certain cases, offers beneficial regularization effects. The efficacy of this exploration is demonstrated with a high-quality regression model, which maintains its accuracy despite the applied quantization. For \gls{ml} portability, the custom \gls{fp} representation is encapsulated within a standard format -- a design feature that the proposed hardware automatically detects and processes. To validate the practicality of this approach, the hardware/software architecture is properly integrated with TensorFlow Lite, demonstrating interoperability with industry-standard \gls{ml} frameworks and affirming the methodology potential for practical, wide-scale deployment in various sensing applications while maintaining compliance with established \gls{ml} infrastructure.


This dissertation addresses a pivotal challenge in the current technological landscape: the harmonization of computational accuracy with energy-efficient and compatible hardware solutions, especially within the context of the rapidly expanding low-power \gls{iot} devices driven by \gls{ai}. This dissertation stands as a significant contribution towards the development of a sustainable, next-generation architecture for neural network processors that are essential in powering the increasingly connected and intelligent world of tomorrow.