\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{silver2016mastering}
\citation{gulshan2016development}
\citation{lake2015human}
\citation{xiong2016achieving}
\citation{brown2020language}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background and Related Work}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {xchapter}{Background and Related Work}{15}{chapter.2}\protected@file@percent }
\@writefile{lot}{\contentsline {xchapter}{Background and Related Work}{15}{chapter.2}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap.background}{{2}{15}{Background and Related Work}{chapter.2}{}}
\newlabel{chap.background@cref}{{[chapter][2][]2}{[1][15][]15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{15}{section.2.1}\protected@file@percent }
\citation{smetters1996synaptic}
\citation{mcdonnell2011benefits}
\citation{ernst2007efficient}
\citation{Dapello2020.06.16.154542}
\citation{davies2018loihi}
\citation{Spinnaker_TransSolid_13}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Spiking Neural Networks}{16}{section.2.2}\protected@file@percent }
\citation{roy2019towards}
\citation{bouvier2019spiking}
\citation{young2019review}
\citation{TrueNorth_Trans15}
\citation{Spinnaker_TransSolid_13}
\citation{davies2018loihi}
\citation{izhikevich2004model}
\citation{amunts2019human}
\citation{gerstner2014neuronal}
\citation{merolla2014million}
\citation{neftci2019surrogate}
\citation{lee2016training}
\citation{neftci2019surrogate}
\citation{masquelier2007unsupervised}
\citation{rotermund2019back}
\citation{ernst2007efficient}
\citation{shadlen1994noise}
\citation{softky1993highly}
\citation{ganguli2012compressed}
\citation{ganguli2010statistical}
\citation{pfeiffer2018deep}
\citation{ernst2007efficient}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Spike-by-Spike Neural Networks}{17}{section*.20}\protected@file@percent }
\newlabel{sec:sbs}{{2.2}{17}{Spike-by-Spike Neural Networks}{section*.20}{}}
\newlabel{sec:sbs@cref}{{[section][2][2]2.2}{[1][17][]17}}
\citation{ernst2007efficient}
\citation{rotermund2019Backpropagation}
\citation{izhikevich2004model}
\newlabel{eq:sbs_update}{{2.2}{18}{Spike-by-Spike Neural Networks}{equation.2.2.2}{}}
\newlabel{eq:sbs_update@cref}{{[section][2][2]2.2}{[1][18][]18}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Basic Network Overview}{18}{section*.21}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces SbS layer update.\relax }}{19}{algocf.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:sbs}{{1}{19}{Basic Network Overview}{algocf.1}{}}
\newlabel{alg:sbs@cref}{{[algocf][1][]1}{[1][18][]19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \gls {sbs} network architecture for handwritten digit classification task.\relax }}{19}{figure.caption.23}\protected@file@percent }
\newlabel{fig:sbs_network}{{2.1}{19}{\gls {sbs} network architecture for handwritten digit classification task.\relax }{figure.caption.23}{}}
\newlabel{fig:sbs_network@cref}{{[figure][1][2]2.1}{[1][18][]19}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces \gls {sbs} network architecture for handwritten digit classification task.\relax }}{19}{table.caption.24}\protected@file@percent }
\newlabel{tab:sbs_network}{{2.1}{19}{\gls {sbs} network architecture for handwritten digit classification task.\relax }{table.caption.24}{}}
\newlabel{tab:sbs_network@cref}{{[table][1][2]2.1}{[1][18][]19}}
\citation{lecun1998mnist}
\citation{rotermund2019Backpropagation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \gls {sbs} \glspl {ip_sbs} as independent computational entities, (a) illustrates an input layer with a massive amount of \glspl {ip_sbs} operating as independent computational entities, (b) shows a hidden layer with an arbitrary amount of \glspl {ip_sbs} as independent computational entities, (c) exhibits a set of neurons grouped in an \gls {ip_sbs}.\relax }}{20}{figure.caption.26}\protected@file@percent }
\newlabel{fig:SbS_layer}{{2.2}{20}{\gls {sbs} \glspl {ip_sbs} as independent computational entities, (a) illustrates an input layer with a massive amount of \glspl {ip_sbs} operating as independent computational entities, (b) shows a hidden layer with an arbitrary amount of \glspl {ip_sbs} as independent computational entities, (c) exhibits a set of neurons grouped in an \gls {ip_sbs}.\relax }{figure.caption.26}{}}
\newlabel{fig:SbS_layer@cref}{{[figure][2][2]2.2}{[1][20][]20}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Computational Cost}{20}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Error Tolerance}{20}{section*.27}\protected@file@percent }
\citation{rosenblatt1958perceptron}
\citation{rumelhart1986learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces (a) Performance classification of \gls {sbs} NN versus equivalent \gls {cnn}, and (b) example of the first pattern in the MNIST test data set with different amounts of positive additive uniformly distributed noise.\relax }}{21}{figure.caption.28}\protected@file@percent }
\newlabel{fig:robustnes_sbs}{{2.3}{21}{(a) Performance classification of \gls {sbs} NN versus equivalent \gls {cnn}, and (b) example of the first pattern in the MNIST test data set with different amounts of positive additive uniformly distributed noise.\relax }{figure.caption.28}{}}
\newlabel{fig:robustnes_sbs@cref}{{[figure][3][2]2.3}{[1][20][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Conventional Artificial Neural Networks}{21}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Architecture}{21}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Layers}{21}{section*.29}\protected@file@percent }
\citation{glorot2010understanding}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Weights and Bias}{22}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Activation Functions}{22}{section*.31}\protected@file@percent }
\citation{maas2013rectifier}
\citation{he2015delving}
\citation{bottou2010large}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Training Process}{24}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Stochastic Gradient Descent}{24}{section*.32}\protected@file@percent }
\citation{bottou2010large}
\citation{bottou2018optimization}
\citation{sutskever2013importance}
\citation{duchi2011adaptive}
\citation{goodfellow2016deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Multi-Layer Perceptron}{25}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Key Components}{25}{section*.33}\protected@file@percent }
\citation{lecun1998gradient}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Convolutional Neural Networks}{26}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Key Components}{26}{section*.34}\protected@file@percent }
\citation{gu2018recent}
\citation{goodfellow2016deep}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Conv2D Tensor Operation}{27}{section*.35}\protected@file@percent }
\newlabel{eq:conv2D}{{2.3}{28}{Conv2D Tensor Operation}{equation.2.3.3}{}}
\newlabel{eq:conv2D@cref}{{[subsection][4][2,3]2.3.4}{[1][28][]28}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Computational Cost of a Convolution Layer}{28}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Definitions}{28}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Computations Per Output Element}{28}{section*.38}\protected@file@percent }
\citation{han2015deep}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Output Dimensions}{29}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Total Computations}{29}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Error Tolerance in Convolution Layers}{29}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Sources of Error}{29}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Error Compensating Mechanisms}{29}{section*.43}\protected@file@percent }
\citation{sze2017efficient}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Exploiting Error Tolerance for Optimization}{30}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Neural Network Accelerators}{30}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}The Need for Accelerators}{30}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Types of Accelerators}{32}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Design Considerations}{33}{subsection.2.4.3}\protected@file@percent }
\citation{micikevicius2017mixed}
\citation{courbariaux2014training}
\citation{jacob2018quantization}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Precision and Effect in Training}{35}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Fixed-Point}{35}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Floating-Point}{36}{subsection.2.5.2}\protected@file@percent }
\newlabel{eq:float}{{2.9}{36}{Floating-Point}{equation.2.5.9}{}}
\newlabel{eq:float@cref}{{[subsection][2][2,5]2.5.2}{[1][36][]36}}
\newlabel{eq:float_bias}{{2.10}{36}{Floating-Point}{equation.2.5.10}{}}
\newlabel{eq:float_bias@cref}{{[subsection][2][2,5]2.5.2}{[1][36][]36}}
\citation{zuras2008ieee}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Floating-point number representation.\relax }}{37}{figure.caption.45}\protected@file@percent }
\newlabel{fig:floating}{{2.4}{37}{Floating-point number representation.\relax }{figure.caption.45}{}}
\newlabel{fig:floating@cref}{{[figure][4][2]2.4}{[1][37][]37}}
\newlabel{eq:float_subnorm}{{2.11}{37}{Floating-Point}{equation.2.5.11}{}}
\newlabel{eq:float_subnorm@cref}{{[subsection][2][2,5]2.5.2}{[1][37][]37}}
\citation{krishnamoorthi2018quantizing}
\citation{krishnamoorthi2018quantizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Post-Training Quantization}{38}{subsection.2.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Quantization-Aware Training}{38}{subsection.2.5.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Custom floating-point quantizer.\relax }}{39}{algocf.2}\protected@file@percent }
\newlabel{alg:quantizr}{{2}{39}{Quantization-Aware Training}{algocf.2}{}}
\newlabel{alg:quantizr@cref}{{[algocf][2][]2}{[1][39][]39}}
\citation{sze2017efficient}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Dataflow Taxonomy}{40}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Flynn's Taxonomy}{41}{section.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Multiply-Accumulate Unit}{42}{section.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.1}Design Considerations}{43}{subsection.2.8.1}\protected@file@percent }
\citation{bouvier2019spiking}
\citation{courbariaux2015binaryconnect}
\citation{han2015deep}
\citation{hubara2017quantized}
\citation{rastegari2016xnor}
\citation{lecun1989optimal}
\citation{hassibi1992second}
\citation{molchanov2016pruning}
\citation{li2016pruning}
\citation{liu2018rethinking}
\citation{moons20160}
\citation{whatmough201714}
\citation{rastegari2016xnor}
\citation{sun2018xnor}
\citation{rathi2018stdp}
\citation{sen2017approximate}
\citation{srivastava2014dropout}
\citation{wan2013regularization}
\citation{neftci2016stochastic}
\citation{srinivasan2016magnetic}
\citation{buesing2011neural}
\citation{bellec2017deep}
\citation{chen20184096}
\citation{sheik2016synaptic}
\citation{jerry2017ultra}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Related Work}{44}{section.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.1}Aggressive Quantization}{44}{subsection.2.9.1}\protected@file@percent }
\citation{zhang2018survey}
\citation{han2013approximate}
\citation{han2013approximate}
\citation{kim2013energy}
\citation{he2016deep}
\citation{russakovsky2015imagenet}
\citation{rastegari2016xnor}
\citation{rotermund2018massively}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.2}Spiking Neural Network Accelerators}{45}{subsection.2.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Classical Approximate Computing}{45}{section*.57}\protected@file@percent }
\citation{lian2019high}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Spike-by-Spike Neural Network Accelerators}{46}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.3}Convolutional Neural Network Accelerators with Custom Floating-Point Computation on \gls {fpga}}{46}{subsection.2.9.3}\protected@file@percent }
\newlabel{sec:related_work}{{2.9.3}{46}{Convolutional Neural Network Accelerators with Custom Floating-Point Computation on \gls {fpga}}{subsection.2.9.3}{}}
\newlabel{sec:related_work@cref}{{[subsection][3][2,9]2.9.3}{[1][46][]46}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline High-Performance FPGA-Based CNN Accelerator With Block-Floating-Point Arithmetic}{46}{section*.59}\protected@file@percent }
\citation{mei2017200mhz}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces (a) System architecture. (b) Processing element array.\relax }}{47}{figure.caption.60}\protected@file@percent }
\newlabel{fig:lian2019high}{{2.5}{47}{(a) System architecture. (b) Processing element array.\relax }{figure.caption.60}{}}
\newlabel{fig:lian2019high@cref}{{[figure][5][2]2.5}{[1][47][]47}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline A 200MHZ 202.4GFLOPS@10.8W VGG16 Accelerator in Xilinx VX690T}{47}{section*.61}\protected@file@percent }
\citation{wu2021low}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces (a) System architecture. (b) Convolution accelerator.\relax }}{48}{figure.caption.62}\protected@file@percent }
\newlabel{fig:mei2017200mhz}{{2.6}{48}{(a) System architecture. (b) Convolution accelerator.\relax }{figure.caption.62}{}}
\newlabel{fig:mei2017200mhz@cref}{{[figure][6][2]2.6}{[1][48][]48}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Low-precision Floating-point Arithmetic for High-performance FPGA-based CNN Acceleration}{48}{section*.63}\protected@file@percent }
\citation{meloni2019cnn}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces (a) System architecture. (b) Processing element.\relax }}{49}{figure.caption.64}\protected@file@percent }
\newlabel{fig:wu2021low}{{2.7}{49}{(a) System architecture. (b) Processing element.\relax }{figure.caption.64}{}}
\newlabel{fig:wu2021low@cref}{{[figure][7][2]2.7}{[1][49][]49}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline CNN Hardware Acceleration on a Low-Power and Low-Cost APSoC}{49}{section*.65}\protected@file@percent }
\citation{wu2020phoenix}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces (a) System architecture. (b) Convolution engine.\relax }}{50}{figure.caption.66}\protected@file@percent }
\newlabel{fig:meloni2019cnn}{{2.8}{50}{(a) System architecture. (b) Convolution engine.\relax }{figure.caption.66}{}}
\newlabel{fig:meloni2019cnn@cref}{{[figure][8][2]2.8}{[1][50][]50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.4}Neural Network Accelerators for Training and Inference with 8-bit Floating-Point Computation on \gls {asic}}{50}{subsection.2.9.4}\protected@file@percent }
\citation{park2021neural}
\citation{venkataramani2021rapid}
\citation{venkataramanaiah202228nm}
\citation{gallus2018handwritten}
\citation{wang2018training}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.5}Academic and Industrial Research on 8-bit Floating-Point Quantization Techniques in Neural Network Training}{51}{subsection.2.9.5}\protected@file@percent }
\citation{sun2019hybrid}
\citation{mellempudi2019mixed}
\citation{liu2021improving}
\@setckpt{./chapters/background}{
\setcounter{page}{53}
\setcounter{equation}{15}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{9}
\setcounter{subsection}{5}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{1}
\setcounter{Item}{55}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{0}
\setcounter{float@type}{8}
\setcounter{parentequation}{0}
\setcounter{etoc@tocid}{1}
\setcounter{etoc@tocdepth}{2}
\setcounter{lstnumber}{1}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{r@tfl@t}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{mtc}{2}
\setcounter{minitocdepth}{1}
\setcounter{ptc}{0}
\setcounter{parttocdepth}{2}
\setcounter{su@anzahl}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{2}
\setcounter{algocfproc}{2}
\setcounter{algocf}{2}
\setcounter{ALC@unique}{35}
\setcounter{ALC@line}{20}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{theorem}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{2}
\setcounter{lstlisting}{0}
\setcounter{minilofdepth}{2}
\setcounter{minilotdepth}{2}
\setcounter{partlofdepth}{2}
\setcounter{partlotdepth}{2}
\setcounter{sectlofdepth}{2}
\setcounter{sectlotdepth}{2}
}
