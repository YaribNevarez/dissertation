\chapter{Introduction}\label{chap.intro}
\minitoc
\section{Preamble}

This section presents the preamble to investigate design methodologies for low-power hardware accelerators of \gls{ai}/\gls{ml} algorithms focusing on inference quality, scalability, versatility, and compatibility as design philosophy.

\subsection{AI/ML in Industry 4.0}
\gls{ai} and \gls{ml} play a crucial role in the context of Industry 4.0, which is characterized by the integration of digital technologies into manufacturing and industrial processes to create a more connected, intelligent, and automated environment.

\subsubsection{Industry 4.0}
Since the beginning of industrialization, technological leaps have led to paradigm shifts, now called "industrial revolutions": from mechanization, electrification, and later, digitalization (the so-called 3rd industrial revolution). Based on the advanced digitalization within factories, the combination of Internet technologies and future-oriented technologies in the field of "smart" things (machines and products) seems to result in a new fundamental paradigm shift in industrial production. Emerging from this future expectation, the term "Industry 4.0" was established for an expected "4th industrial revolution"~\cite{lasi2014industry}.


\subsubsection{Internet-of-Things in Industry}
% Context background
To build the emerging environment of Industry 4.0, disruptive technologies are required to handle autonomous communications between all industrial embedded computers throughout the factory and the Internet. Such technologies offer the potential to transform the industry along the entire production chain and stimulate productivity and overall economic growth \cite{espinoza2020estimating}. These technologies include cloud computing, big data, and specially a new generation of \gls{iot} devices fused with \gls{cps}, safety-security, augmented reality, \gls{ml}, and hardware accelerators~\cite{alcacer2019scanning}.

\subsection{Rationale for \gls{ai}/\gls{ml} Acceleration in \gls{iot} Applications}
% Particular background
The continuous evolution of \gls{ai} algorithms and \gls{iot} devices has not only made \gls{ai} the major workload running on these embedded devices, but has transformed \gls{ai} into the main approach for industrial solutions, especially in the rise of Industry 4.0~\cite{alcacer2019scanning}. As a result, the term of \gls{iot} has also been redefined as \gls{ai} of Things (AIoT) to emphasize the impact on this technology~\cite{zhang2020empowering}.

There are key motivations for accelerating \gls{ai}/\gls{ml} algorithms within \gls{iot} devices, focusing on mission criticality, real-time processing, data privacy and security, and offline operation capabilities~\cite{loh20201}:

\subsubsection{Mission Criticality}
In mission-critical applications such as medical devices, autonomous vehicles, and industrial automation, the need for quick and reliable decisions is essential. Low-power neural network accelerators allow these devices to make decisions in real time without significant power consumption. These accelerators can be designed to meet stringent safety and reliability standards, reducing the risk of failures in critical applications.

\subsubsection{Real-Time Processing}
For applications such as autonomous vehicles, drones, and robotics, immediate processing of sensor data (such as images and LIDAR data) is necessary. Low-power accelerators can process this data on-device in real time, reducing latency compared to cloud-based solutions. Real-time processing is crucial for responsive and adaptive system behavior, which is essential for the smooth functioning of \gls{iot} systems.

\subsubsection{Data Privacy and Security}
Processing data on-device, rather than sending it to a central server or cloud, can substantially mitigate the risk of data interception or manipulation. With the increasing scrutiny and regulations around data privacy (e.g., \gls{gdpr}), processing data locally on a device is a critical advantage, enabling compliance with data protection laws. For sensitive applications such as smart home devices or wearables that handle personal data, on-device processing with low-power neural network accelerators maintains user privacy.

\subsubsection{Offline Operation Capabilities}
Low-power neural network accelerators enable IoT and embedded devices to operate independently of network connectivity, allowing for effective functioning in remote or disconnected environments. These accelerators allow for continuous operation without reliance on a central server, which is critical in situations where connectivity is inconsistent, costly, or non-existent, such as in agricultural or maritime contexts.

\subsubsection{Energy Efficiency}
For battery-powered or energy-harvesting devices, the power consumption of the processing unit is a critical constraint. Low-power accelerators are optimized for energy efficiency, which is essential for prolonging device operational lifetime without frequent battery replacements or recharging.

\subsubsection{Emerging Applications}
\begin{itemize}
	\item \textbf{Edge AI:} This is where \gls{ai} algorithms are processed locally on a hardware device. The algorithms are run locally, on a hardware chip, without requiring a connection to a network, unlike in cloud \gls{ai} where algorithms run in a data center. Low-power accelerators are key enablers of this paradigm.
	\item \textbf{TinyML:} \gls{tinyml} is the deployment of machine learning algorithms on low-power hardware, such as microcontrollers. These devices are particularly relevant in \gls{iot} applications where power, cost, and form factor are key considerations.
	\item \textbf{Predictive Maintenance:} In industrial \gls{iot} applications, low-power neural network accelerators can be used to continuously monitor the health of machinery and predict when maintenance is required in a reliable and energy efficient manner.
	\item \textbf{Health Monitoring:} Continuous health and wellness monitoring through wearable devices is an emerging application. For example, real-time analysis of ECG or other biometric data can be performed efficiently on-device using low-power accelerators.
	\item \textbf{Smart Agriculture:} These devices can be used for precision farming, where they analyze data from various sensors in real time and make decisions to optimize farming practices.
	\item \textbf{Natural Language Processing:} In consumer devices, such as smart speakers or smartphones, low-power neural network accelerators enable more efficient and responsive voice recognition and processing.
	\item \textbf{Federated Learning:} Low-power accelerators can facilitate \gls{fl} by efficiently handling the computations required for local model training and updates, thereby contributing to both data privacy and security.
\end{itemize}



\subsection{Approximation in AI/ML}
Based on the error tolerance in \gls{ml} algorithms, a promising solution is approximate computing. This paradigm has been used in a wide range of applications to increase hardware efficiency~\cite{han2013approximate}. For neural network applications, two main approximation strategies are used, namely network compression and classical approximate computing~\cite{bouvier2019spiking}.

\subsubsection{Network Compression and Quantization}
Researchers focusing on embedded applications started lowering the precision of weights and activation maps to shrink the memory footprint of the large number of parameters representing \glspl{ann}, a method known as network quantization. In this manner, reduced bit precision causes a small accuracy loss~\cite{courbariaux2015binaryconnect, han2015deep, hubara2017quantized, rastegari2016xnor}. In addition to quantization, network pruning reduces the model size by removing structural portions of the parameters and its associated computations~\cite{lecun1989optimal,hassibi1992second}. This method has been identified as an effective technique to improve the efficiency of neural network models for applications with limited computational and energy budget~\cite{molchanov2016pruning,li2016pruning, liu2018rethinking}. These techniques leverage the intrinsic error-tolerance of neural networks, as well as their ability to recover from accuracy degradation while training.

\subsubsection{Error Tolerance in AI/ML Algorithms}
An algorithm can be regarded as error-tolerant or error-resilient when it provides a result with the required accuracy while utilizing processing components with a certain degree of inaccuracy. There are several reasons why an algorithm/application is tolerant of errors as discussed in~\cite{chippa2013analysis}. These include noisy or redundant data of the algorithm, approximate or probabilistic computations within the algorithm, and a range of acceptable outcomes. This is the case of \gls{ai}/\gls{ml} models.


\subsubsection{Approximate Computing}
%Geneal background
Approximate computing is a design paradigm that is able to tradeoff computation quality (e.g., accuracy) and computational efficiency (e.g., in run-time, chip-area, and/or energy) by exploiting the error resilience of applications/algorithms~\cite{gillani2020exploiting, zhang2015approxann}. Data redundancy of neural networks incorporate a certain degree of resilience against random external and internal perturbations;
for instance, noisy inputs and random hardware errors. This property can be exploited in a cross-layer resilience approach~\cite{carter2010design}: by leveraging error tolerance at algorithmic-level, it can be allowed a certain degree of inaccuracies at the computing-level. This approach consists of designing processing elements that approximate their computation by employing cleverly modified algorithmic logic units~\cite{han2013approximate}.

Approximate computing techniques allow substantial enhancement in processing efficiency with moderated accuracy degradation. Some research papers have shown the feasibility of applying approximate computing to the inference stage of neural networks~\cite{lotrivc2012applicability, han2013approximate, du2014leveraging, mrazek2016design, sarwar2016multiplier, zervakis2021approximate}. Such techniques usually demonstrated small inference accuracy degradation, but significant enhancement in computational performance, chip-area, and energy consumption. Hence, by taking advantage of the intrinsic error-tolerance of neural networks, approximate computing is positioned as a promising approach for \gls{ai}/\gls{ml} computation on resource-limited devices.

% Problem to solve
\section{Problem Statement}
A fundamental problem for the rise of \gls{ai} in Industry 4.0 is the fact that \gls{ml} models, are highly computational and data intensive. This brings significant challenges across the spectrum of computing hardware, specially in the scope of embedded systems~\cite{venkataramani2016efficient}. The most deployed models and also some of the most computationally and energy expensive are \glspl{cnn} for computer vision applications. Compared to the conventional image processing methods, the accuracy of \gls{cnn} has improved significantly that by 2015, a human can no longer beat a computer in image classification~\cite{loh20201}. The early development of \glspl{cnn} before 2016 mainly focused on accuracy enhancement without considering computational costs. While accuracy of deep \gls{cnn} for image classification improved 24\% between 2012 and 2016, the demand on hardware resources increased more than $10\times$. Starting from 2017, significant attention was paid to improve hardware efficiency in terms of compute power, memory bandwidth, and power consumption, while maintaining accuracy at a similar level to human perception~\cite{venkataramani2016efficient}.

\subsection{Power Dissipation}
Consequently, the recent breakthroughs in \gls{ai}/\gls{ml} applications have brought significant advancements in neural network processors~\cite{jouppi2017datacenter}. To bring the inference speed to an acceptable level, \gls{asic} with \gls{npu} are becoming ubiquitous in both embedded and general purpose computing. \glspl{npu} perform several tera operations per second in a confined area, as a consequence, they become subject to elevated on-chip power densities that rapidly result in excessive on-chip temperatures during operation~\cite{amrouch2020npu}. Subsequently, the elevated power supply, physical dimensions, heat sink and air cooling requirements demand a balance between the benefits of \gls{ml} against its financial and environmental costs. This outcome is delivered by parallel computing techniques, yet unsustainable in resource-constrained devices.
Therefore, radical changes to conventional computing are required in order to sustain and improve performance while satisfying energy and temperature constraints~\cite{gillani2020exploiting}.

\subsection{Aggressive Quantization}
Furthermore, reducing the compute hardware with aggressive quantization such as binary \cite{courbariaux2015binaryconnect}, ternary \cite{lin2015neural}, and mixed precision (2-bit activations and ternary weights) \cite{colangelo2018exploration} typically incur significant accuracy degradation for very low precisions, especially for complex problems~\cite{faraone2019addnet}, such as: regression, semantic segmentation, machine translation, language generation, playing agents, image/music generation, and medical applications.

While aggressive quantization can be beneficial for resource-constrained environments and non-critical applications, careful consideration and a more conservative approach are essential for ensuring the safety and reliability of \gls{ai}/\gls{ml} systems in high-accuracy or mission-critical domains. Quantization techniques must be chosen wisely, keeping in mind the specific requirements and constraints of each application.

\subsection{Interoperability}
Aggressive or exotic quantization might not be supported by all hardware/software platforms. Custom hardware accelerators may have limitations on the precision they can handle effectively, limiting the compatibility and portability of aggressively quantized models. Aggressively quantized models may not be compatible with all frameworks, libraries, or \gls{ai} platforms, limiting their interoperability and portability across different environments. In real-world deployment scenarios, there may be constraints and requirements that make aggressive quantization impractical, especially when high-accuracy, compatibility, portability, and interoperability are necessary.
%%%%%%%%%%%%%%
\section{Working Hypothesis}

The primary hypothesis guiding this research is as follows:

\begin{quote}
	\textit{Implementing a mixed or hybrid precision approach in neural network accelerators -- using reduced \gls{fp} bit-width for weights and biases while retaining standard \gls{fp} for activation maps -- can substantially decrease power consumption and thermal dissipation, without significantly compromising inference accuracy.}
\end{quote}

Under this central hypothesis, the proposed neural network accelerator design, which adopts this hybrid precision approach, is postulated to achieve the following specific outcomes:

\begin{enumerate}[label={\textbf{H\arabic*}}]
	\item \textbf{Efficient Hardware Operation:} Minimization of power consumption and thermal output, enabling sustained and efficient hardware operation~\cite{lai2017deep}.
	\item \textbf{High Inference Quality:} Preservation of high-quality inference results by maintaining standard \gls{fp} precision in activation maps, despite reduced precision in weights and biases.
	\item \textbf{Enhanced Hardware Efficiency:} Improvement in the hardware efficiency of the accelerator, facilitated by faster arithmetic operations and decreased memory bandwidth requirements, due to the hybrid precision approach~\cite{lai2017deep}.
	\item \textbf{Compatibility and Interoperability:} Ability to efficiently handle various precision levels for weights, biases, and activation maps, ensuring compatibility and integration across diverse industry standard software frameworks and hardware platforms.
	\item \textbf{Accuracy Preservation via \gls{qat}:} Utilization of \gls{qat} to adapt neural network models to the hybrid precision, maintaining high model accuracy despite reduced precision components.
\end{enumerate}

\paragraph{Implications}
This research seeks to demonstrate that the strategic application of hybrid \gls{fp} precision computation in neural network accelerators can enable a new paradigm of energy-efficient machine learning hardware without sacrificing quality, reliability, and interoperability.



%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Objective}

The overarching objective of this research is to:

\begin{quote}
	\textit{Develop advanced methodologies for energy-efficient neural network accelerators that utilize custom \gls{fp} computation in resource-constrained environments.}
\end{quote}

This primary objective can be decomposed into specific sub-objectives and key aspects, which are enumerated as follows:

\begin{enumerate}[label={\textbf{O\arabic*}}]
	\item \textbf{Optimized Custom \gls{fp} Representation:} 
	Examine custom \gls{fp} representations that are minimal specifically tailored for neural network computations, involving various non-standard \gls{fp} formats with different bit-widths for the exponent and mantissa.
	
	\item \textbf{Energy-Efficient Design Strategies:} 
	Investigate design strategies from logic-level optimizations to architectural-level approaches, with the primary aim of minimal energy consumption.
	
	\item \textbf{Custom \gls{fp} Arithmetic Units:} 
	Research the design and implementation of arithmetic units optimized for energy-efficient execution of proposed custom \gls{fp} computations.
	
	\item \textbf{Task-Specific Hardware Optimization:} 
	Study accelerator architectures specialized for neural network tasks in environments with constrained resources, employing techniques such as pipelining and hardware-specific optimizations to ensure efficiency and high \gls{qor}.
	
	\item \textbf{Precision and Quantization Impact Analysis:} 
	Thoroughly explore and analyze the effects of quantization and reduced precision on model accuracy, employing \gls{qat} and dynamic precision techniques to preserve or enhance accuracy.
	
	\item \textbf{Scalability and Flexibility:} 
	Investigate scalability solutions that can adeptly handle diverse neural network models and sizes, promoting broader applicability.
	
	\item \textbf{Comparative Analysis with Alternative Techniques:} 
	Conduct comparisons with other energy-efficient neural network accelerators to emphasize the unique advantages and strengths of the proposed custom \gls{fp} computation approaches.
	
	\item \textbf{Performance Evaluation and Benchmarking:} 
	Perform rigorous evaluation and benchmarking against the existing state-of-the-art, aiming to highlight the benefits in terms of energy efficiency and computational performance.
	
	\item \textbf{Practical Application Demonstrations:} 
	Showcase the practical applicability of the proposed methodologies in low-power and resource-constrained scenarios, such as \gls{iot} devices and sensor analytics.
	
	\item \textbf{Directions for Future Research:} 
	Offer valuable insights into potential future research paths for further refining energy-efficient neural network accelerators with custom \gls{fp} computation for learning purposes.
\end{enumerate}


\paragraph{Core Aims}
\begin{itemize}
	\item \textbf{Quality Preservation}: Ensure that reduced precision does not compromise the accuracy and reliability.
	\item \textbf{Versatility}: Design accelerators adaptable to diverse neural network tasks and deployment scenarios.
	\item \textbf{Compatibility}: Facilitate integration of the proposed accelerator designs with existing systems and software frameworks.
\end{itemize}

Overall, this research aims to significantly contribute to the evolving development of energy-efficient neural network accelerators, targeting an advancement of the state-of-the-art in this field.

\section{Scope}\label{chap1.scope}

The scope of this research encompasses the development of energy-efficient and high-quality inference mechanisms for \gls{sbs} and \gls{cnn} models in resource-constrained applications. The methodologies are particularly tailored for deployment on \gls{soc} devices, which operate under stringent computational, memory, and power constraints.

\begin{enumerate}[label={\textbf{S\arabic*}}]
	
	\item \textbf{Spike-by-Spike Neural Networks}
	\begin{itemize}
		\item \glspl{snn} offer advantageous robustness and the potential to achieve a power efficiency closer to that of the human brain. 
		\item They operate reliably using stochastic elements that are inherently non-reliable mechanisms~\cite{mcdonnell2011benefits}. This provides superior resistance against adversary attacks~\cite{ernst2007efficient, Dapello2020.06.16.154542}.
		\item The Spike-by-Spike model is on the less realistic side of the \gls{snn} scale of biological realism~\cite{rotermund2019Backpropagation,ernst2007efficient}. Consequently, the hardware complexity of \gls{sbs} network implementations is greatly reduced~\cite{rotermund2018massively}.
		\item Despite the reduced complexity, \gls{sbs} retains the advantageous robustness of \glspl{snn} through its use of stochastic spikes for transmitting information between neuron populations.
		\item However \gls{sbs} models present elevated computational demands and memory footprint, unsuitable for resource-constrained environments. Hence, these models have not been investigated in low-power applications. \gls{sbs} accelerators can facilitate neuroscience research~\cite{ernst2007efficient,rotermund2019recurrentsbs, dayan2001theoretical} and contribute to deploying robust neural networks in small embedded systems~\cite{nevarez2020accelerator}.
	\end{itemize}
	
	\item \textbf{Convolutional Neural Networks}
	\begin{itemize}
		\item \glspl{cnn} are the essential building blocks in 2D pattern analytics. They have been employed in sensor-based applications, such as mechanical fault detection, structural health monitoring, \gls{har}, and hazardous gas detection, both in industry and academia~\cite{li2019sensor,dong2018rolling,nagayama2007structural,wang2019deep,kim2017hazardous}.
		\item \gls{cnn} models provide advantages such as local dependency, translation invariance, and noise resilience in analytics~\cite{du2014leveraging}.
		\item These models, however, are computationally intensive and power-hungry, posing challenges for low-power embedded applications, particularly in the field of \gls{iot} and sensor analytics.
		\item Numerous commercial \gls{asic} and \gls{fpga} accelerators have been proposed for data-centers and embedded systems applications. However, most target mid- to high-range \glspl{fpga} and exhibit drawbacks, such as high power supply demands, physical dimensions, cooling requirements, and cost.
		\item Aggressive quantization (e.g., binary, ternary, and mixed precision implementations) often incurs significant accuracy degradation, especially for complex problems~\cite{courbariaux2015binaryconnect,lin2015neural,colangelo2018exploration,faraone2019addnet}.
		\item These limitations prevent widespread applicability in scenarios where low-power, accuracy, and interoperability are mandatory.
	\end{itemize}
	
\end{enumerate}

\section{Contributions}

This research produces hardware design methodologies for low-power hardware accelerators with custom \gls{fp} computation that reconcile efficiency with inference quality, and compatibility. This work is demonstrated on \gls{sbs} and \gls{cnn} hardware accelerators on resource-constrained \gls{soc} \glspl{fpga}.

\subsection{Accelerating Spike-by-Spike Neural Networks with Hybrid 8-bit Floating-Point and 4-bit Logarithmic Computation}
\begin{enumerate}
	\item \textbf{Optimized \gls{mac} Design}: 
	An optimized \gls{fp} \gls{mac} design with hybrid precision is presented. It utilizes the IEEE 754 single-precision \gls{fp} for feature maps and a custom \gls{fp} representation for weights, which significantly enhances efficiency through reduced latency, minimized hardware resources, and a smaller memory footprint, while maintaining \gls{qor}.
	
	\item \textbf{Design Exploration and Evaluation}: 
	A comprehensive design exploration for 8 and 4-bit input weight \gls{fp} representation is detailed. Evaluations report runtime, accuracy degradation, hardware resource utilization, and power consumption. A significant latency improvement and minimal accuracy degradation were measured.
	
	\item \textbf{Quality Monitoring through Noise Tolerance Plot}: 
	A noise tolerance plot is proposed as a quality monitor, intended to serve as an intuitive visual model that provides insights into the accuracy degradation of \gls{sbs} networks when subjected to custom \gls{fp} computation.
	
	\item \textbf{Adaptable Design for Error-Resilient Applications}: 
	The custom \gls{fp} \gls{mac} design is proposed as adaptable for use as a building block in other error-resilient applications, such as image/video processing.
\end{enumerate}

\subsection{Accelerating Convolutional Neural Networks with Hybrid 6-bit Floating-Point Computation}
\begin{enumerate}
	\item \textbf{\gls{hf6} Quantization and \gls{mac} Design}:
	This research introduces a 6-bit \gls{fp} representation tailored specifically for weights and bias. The proposed hardware \gls{mac} unit integrates the IEEE 754 single-precision \gls{fp} used for feature maps and a custom \gls{fp} scheme designed for weights. This combination boost efficiency in multiple dimensions: a marked reduction in latency through denormalized accumulation, hardware area reduction achieved by changing traditional mantissa multiplication with a more efficient multiplexer-adder operation, and optimized memory usage. The \gls{qor} is retained throughout these enhancements. This balance is further augmented by the incorporation of the \gls{qat} approach to assure model accuracy.
	
	\item \textbf{Custom Hardware/Software Co-design Framework}: 
	A co-design framework for \gls{cnn} sensor analytics applications has been developed, targeting resource-constrained \gls{soc} \glspl{fpga}. This architecture incorporates TensorFlow Lite.
	
	\item \textbf{Customizable Tensor Processor with \gls{hf6}}: 
	A customizable \gls{tp} is demonstrated as a proof of concept with \gls{hf6}. Significant acceleration is achieved for the \emph{Conv2D} tensor operation without accuracy degradation employing \gls{qat}.
	
	\item \textbf{Demonstration and Design Exploration}: 
	The potential of this approach is showcased with a \gls{cnn}-regression model for anomaly localization in \gls{shm} based on \gls{ae}. A hardware design exploration evaluates accuracy, compute performance, hardware resource utilization, and energy consumption.
\end{enumerate}


\section{Publications}
The outcome of this dissertation, including the collaborative works with our research partners is a list of publications. In the following, a complete list of the related publications is itemized.

\begin{enumerate}
	
	\subsection*{Journal Articles}
	
	\item \textbf{Yarib Nevarez}, David Rotermund, Klaus R Pawelzik, and Alberto Garcia-Ortiz, "Accelerating Spike-by-Spike Neural Networks on FPGA With Hybrid Custom Floating-Point and Logarithmic Dot-Product Approximation," 
	\newblock IEEE Access, vol. 9, pp. 80603--80620, May 2021, doi: 10.1109/ACCESS.2021.3085216.  
	
	\item \textbf{Yarib Nevarez}, Andreas Beering, Amir Najafi, Ardalan Najafi, Wanli Yu, Yizhi Chen, Karl-Ludwig Krieger, and Alberto Garcia-Ortiz, "CNN Sensor Analytics With Hybrid-Float6 Quantization on Low-Power Embedded FPGAs," 
	\newblock IEEE Access, vol. 11, pp. 4852--4868, January 2023, doi: 10.1109/ACCESS.2023.3235866.

	
	\subsection*{Conference Proceedings}
	
	\item \textbf{Yarib Nevarez}, Alberto Garcia-Ortiz, David Rotermund, and Klaus R Pawelzik, "Accelerator framework of spike-by-spike neural networks for inference and incremental learning in embedded systems,"
	\newblock 2020 9th International Conference on Modern Circuits and Systems Technologies (MOCAST), Bremen, 2020, pp. 1--5, doi: 10.1109/MOCAST49295.2020.9200288.
	
	\item Wanli Yu, Ardalan Najafi, \textbf{Yarib Nevarez}, Yanqiu Huang and Alberto Garcia-Ortiz, "TAAC: Task Allocation Meets Approximate Computing for Internet of Things," 
	\newblock 2020 IEEE International Symposium on Circuits and Systems (ISCAS), Sevilla, 2020, pp. 1-5, doi: 10.1109/ISCAS45731.2020.9180895.
	
	\item Amir Najafi, Ardalan Najafi, \textbf{Yarib Nevarez} and Alberto Garcia-Ortiz, "Learning-Based On-Chip Parallel Interconnect Delay Estimation," 
	\newblock 2022 11th International Conference on Modern Circuits and Systems Technologies (MOCAST), Bremen, 2022, pp. 1--5, doi: 10.1109/MOCAST49295.2020.9200288.
	
	\item Yizhi Chen, \textbf{Yarib Nevarez}, Zhonghai Lu, and Alberto Garcia-Ortiz, "Accelerating Non-Negative Matrix Factorization on Embedded FPGA with Hybrid Logarithmic Dot-Product Approximation," 
	\newblock 2022 IEEE 15th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC), Malaysia, 2022, pp. 239--246, doi: 10.1109/MCSoC57363.2022.00070.
	
	\item Ardalan Najafi, Wanli Yu, \textbf{Yarib Nevarez}, Amir Najafi, Andreas Beering, Karl-Ludwig Krieger, and Alberto Garcia-Ortiz, "Acoustic Emission Source Localization using Approximate Discrete Wavelet Transform," \newblock 2023 12th International Conference on Modern Circuits and Systems Technologies (MOCAST), Bremen, 2023, pp. 1--5, doi: 10.1109/MOCAST57943.2023.10176952.
	
\end{enumerate}

\section{Dissertation Outline}

This dissertation is organized into three main parts: an introduction that lays the groundwork for understanding the challenges and motivations of the topic; a central core that details the specific methodologies and results obtained in the study; and a final part that includes the conclusions drawn from the research and potential future works. More specifically:

\begin{enumerate}[I]
	\item \textbf{Introduction}: 
	\begin{itemize}
		\item Chapter~\ref{chap.intro}: Introduction to \gls{ai} in the current industrial context and its implication in the low-power \gls{iot} landscape - Describes the current technological landscape where \gls{ai} is integrated into omnipresent devices, highlighting the importance of sustainable and efficient design.
		\item Chapter~\ref{chap.background}: Background on neural networks and hardware acceleration with custom \gls{fp} computation - A dive into the concepts and inherent challenges posed by these algorithms, especially in terms of computational and energy demands.
	\end{itemize}
	
	\item \textbf{Core}: 
	\begin{itemize}
		\item Chapter~\ref{chap.sbs}: This chapter delves into the design of low-power \gls{sbs} neural network accelerators for embedded systems, emphasizing their advantages and challenges, and introduces an innovative hardware \gls{mac} module that blends various \gls{fp} formats to optimize computational accuracy and efficiency, facilitating the deployment of \gls{sbs} networks in resource-limited settings.
		\item Chapter~\ref{chap.sbs}: This chapter presents an innovative hardware design for efficient low-power \gls{cnn} inference in sensor analytics, highlighting the \gls{hf6} quantization method that merges standard \gls{fp} with 6-bit \gls{fp}, and integrates harmoniously with TensorFlow Lite, demonstrating its applicability and alignment with industry standard \gls{ml} platforms.
	\end{itemize}
	
	\item \textbf{Conclusions}: 
	\begin{itemize}
		\item Chapter~\ref{chap.conclusion}: Reflections and future directions - Summarizes the key findings of the research, its implications in the field of low-power neural network accelerators, and the potential avenues for future research and development.
	\end{itemize}
\end{enumerate}
