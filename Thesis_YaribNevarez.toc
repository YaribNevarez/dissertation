\@ifundefined {etoctocstyle}{\let \etoc@startlocaltoc \@gobble \let \etoc@settocdepth \@gobble \let \etoc@depthtag \@gobble \let \etoc@setlocaltop \@gobble }{}
\babel@toc {english}{}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Preamble}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}AI/ML in Industry 4.0}{1}{subsection.1.1.1}%
\contentsline {subsubsection}{\nonumberline Industry 4.0}{1}{section*.4}%
\contentsline {subsubsection}{\nonumberline Internet-of-Things in Industry}{2}{section*.5}%
\contentsline {subsection}{\numberline {1.1.2}Rationale for \gls {ai}/\gls {ml} Acceleration in \gls {iot} Applications}{2}{subsection.1.1.2}%
\contentsline {subsubsection}{\nonumberline Mission Criticality}{2}{section*.6}%
\contentsline {subsubsection}{\nonumberline Real-Time Processing}{2}{section*.7}%
\contentsline {subsubsection}{\nonumberline Data Privacy and Security}{3}{section*.8}%
\contentsline {subsubsection}{\nonumberline Offline Operation Capabilities}{3}{section*.9}%
\contentsline {subsubsection}{\nonumberline Energy Efficiency}{3}{section*.10}%
\contentsline {subsubsection}{\nonumberline Emerging Applications}{3}{section*.11}%
\contentsline {subsection}{\numberline {1.1.3}Approximation in AI/ML}{4}{subsection.1.1.3}%
\contentsline {subsubsection}{\nonumberline Network Compression and Quantization}{4}{section*.12}%
\contentsline {subsubsection}{\nonumberline Error Tolerance in AI/ML Algorithms}{5}{section*.13}%
\contentsline {subsubsection}{\nonumberline Approximate Computing}{5}{section*.14}%
\contentsline {section}{\numberline {1.2}Problem Statement}{5}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Power Dissipation}{6}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Aggressive Quantization}{6}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Interoperability}{7}{subsection.1.2.3}%
\contentsline {section}{\numberline {1.3}Working Hypothesis}{7}{section.1.3}%
\contentsline {paragraph}{\nonumberline Implications}{8}{section*.15}%
\contentsline {section}{\numberline {1.4}Research Objective}{8}{section.1.4}%
\contentsline {paragraph}{\nonumberline Core Aims}{9}{section*.16}%
\contentsline {section}{\numberline {1.5}Scope}{9}{section.1.5}%
\contentsline {section}{\numberline {1.6}Contributions}{11}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Accelerating Spike-by-Spike Neural Networks with Hybrid 8-bit Floating-Point and 4-bit Logarithmic Computation}{11}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}Accelerating Convolutional Neural Networks with Hybrid 6-bit Floating-Point Computation}{11}{subsection.1.6.2}%
\contentsline {section}{\numberline {1.7}Publications}{12}{section.1.7}%
\contentsline {section}{\numberline {1.8}Dissertation Outline}{13}{section.1.8}%
\contentsline {chapter}{\numberline {2}Background and Related Work}{15}{chapter.2}%
\contentsline {section}{\numberline {2.1}Introduction}{15}{section.2.1}%
\contentsline {section}{\numberline {2.2}Neural Networks}{16}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Architecture}{16}{subsection.2.2.1}%
\contentsline {subsubsection}{\nonumberline Layers}{16}{section*.19}%
\contentsline {subsubsection}{\nonumberline Weights and Bias}{17}{section*.20}%
\contentsline {subsubsection}{\nonumberline Activation Functions}{17}{section*.21}%
\contentsline {subsection}{\numberline {2.2.2}Training Process}{19}{subsection.2.2.2}%
\contentsline {subsubsection}{\nonumberline Stochastic Gradient Descent}{19}{section*.22}%
\contentsline {subsection}{\numberline {2.2.3}Multi-Layer Perceptron}{20}{subsection.2.2.3}%
\contentsline {subsubsection}{\nonumberline Key Components}{20}{section*.23}%
\contentsline {subsection}{\numberline {2.2.4}Convolutional Neural Networks}{21}{subsection.2.2.4}%
\contentsline {subsubsection}{\nonumberline Key Components}{21}{section*.24}%
\contentsline {subsubsection}{\nonumberline Conv2D Tensor Operation}{22}{section*.25}%
\contentsline {subsubsection}{\nonumberline Computational Cost of a Convolution Layer}{22}{section*.26}%
\contentsline {paragraph}{\nonumberline Definitions}{23}{section*.27}%
\contentsline {paragraph}{\nonumberline Computations Per Output Element}{23}{section*.28}%
\contentsline {paragraph}{\nonumberline Output Dimensions}{23}{section*.29}%
\contentsline {paragraph}{\nonumberline Total Computations}{23}{section*.30}%
\contentsline {subsubsection}{\nonumberline Error Tolerance in Convolution Layers}{24}{section*.31}%
\contentsline {paragraph}{\nonumberline Sources of Error}{24}{section*.32}%
\contentsline {paragraph}{\nonumberline Error Compensating Mechanisms}{24}{section*.33}%
\contentsline {paragraph}{\nonumberline Exploiting Error Tolerance for Optimization}{24}{section*.34}%
\contentsline {subsection}{\numberline {2.2.5}Spiking Neural Networks}{25}{subsection.2.2.5}%
\contentsline {subsubsection}{\nonumberline Spike-by-Spike Neural Networks}{25}{section*.35}%
\contentsline {paragraph}{\nonumberline Basic Network Overview}{26}{section*.36}%
\contentsline {paragraph}{\nonumberline Computational Cost}{28}{section*.40}%
\contentsline {paragraph}{\nonumberline Error Tolerance}{28}{section*.42}%
\contentsline {section}{\numberline {2.3}Neural Network Accelerators}{28}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}The Need for Accelerators}{29}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Types of Accelerators}{31}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Design Considerations}{32}{subsection.2.3.3}%
\contentsline {section}{\numberline {2.4}Precision}{33}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Fixed-Point}{33}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Floating-Point}{34}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Post-Training Quantization}{36}{subsection.2.4.3}%
\contentsline {subsection}{\numberline {2.4.4}Quantization-Aware Training}{36}{subsection.2.4.4}%
\contentsline {section}{\numberline {2.5}Dataflow Taxonomy}{37}{section.2.5}%
\contentsline {section}{\numberline {2.6}Multiply-Accumulate Unit}{39}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Design Considerations}{39}{subsection.2.6.1}%
\contentsline {section}{\numberline {2.7}Related Work}{40}{section.2.7}%
\contentsline {subsection}{\numberline {2.7.1}Aggressive Quantization}{41}{subsection.2.7.1}%
\contentsline {subsection}{\numberline {2.7.2}Spiking Neural Network Accelerators}{41}{subsection.2.7.2}%
\contentsline {subsubsection}{\nonumberline Classical Approximate Computing}{41}{section*.50}%
\contentsline {subsubsection}{\nonumberline Spike-by-Spike Neural Network Accelerators}{42}{section*.51}%
\contentsline {subsection}{\numberline {2.7.3}Convolutional Neural Network Accelerators with Custom Floating-Point Computation on \gls {fpga}}{42}{subsection.2.7.3}%
\contentsline {subsubsection}{\nonumberline Hybrid Custom Floating-Point}{42}{section*.52}%
\contentsline {subsubsection}{\nonumberline Low-Precision Floating-Point}{43}{section*.53}%
\contentsline {subsubsection}{\nonumberline Low-Power Architectures for \gls {iot} Deployments}{43}{section*.54}%
\contentsline {subsection}{\numberline {2.7.4}Neural Network Accelerators for Training and Inference with 8-bit Floating-Point Computation on \gls {asic}}{43}{subsection.2.7.4}%
\contentsline {subsection}{\numberline {2.7.5}Training Techniques with 8-bit Floating-Point Quantization}{44}{subsection.2.7.5}%
\contentsline {chapter}{\numberline {3}Low-Power Spike-by-Spike Neural Network Accelerator: Hybrid 8-bit Floating-Point and 4-bit Logarithmic Computation}{47}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduction}{48}{section.3.1}%
\contentsline {section}{\numberline {3.2}Design Technique}{51}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Hardware Architecture}{52}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Conv Processing Unit}{52}{subsection.3.2.2}%
\contentsline {subsubsection}{\nonumberline Configuration Mode}{53}{section*.59}%
\contentsline {subsubsection}{\nonumberline Computation Mode}{54}{section*.60}%
\contentsline {subsection}{\numberline {3.2.3}Hybrid Custom Floating-Point Multiply-Accumulate Unit: Vector Dot-Product Approximation}{54}{subsection.3.2.3}%
\contentsline {subsubsection}{\nonumberline Dot-Product with Standard Floating-Point Computation}{56}{section*.62}%
\contentsline {subsubsection}{\nonumberline Dot-Product with Hybrid Custom Floating-Point and Logarithmic Computation}{57}{section*.64}%
\contentsline {section}{\numberline {3.3}Experimental Results}{60}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Performance Benchmark}{61}{subsection.3.3.1}%
\contentsline {subsubsection}{\nonumberline Benchmark on Embedded CPU}{61}{section*.67}%
\contentsline {subsubsection}{\nonumberline Benchmark on Processing Units with Standard Floating-Point Computation}{61}{section*.70}%
\contentsline {subsubsection}{\nonumberline Benchmark on Noise Tolerance Plot}{65}{section*.77}%
\contentsline {subsection}{\numberline {3.3.2}Design Exploration with Hybrid Custom Floating-Point and Logarithmic Computation}{65}{subsection.3.3.2}%
\contentsline {subsubsection}{\nonumberline Parameters for Numeric Representation of Synaptic Weight Matrix}{66}{section*.79}%
\contentsline {subsubsection}{\nonumberline Design Exploration for Dot-product with Hybrid Custom Floating-Point Computation}{67}{section*.81}%
\contentsline {subsubsection}{\nonumberline Design Exploration for Dot-Product whit Hybrid Logarithmic Computation}{68}{section*.86}%
\contentsline {subsection}{\numberline {3.3.3}Results and Discussion}{70}{subsection.3.3.3}%
\contentsline {section}{\numberline {3.4}Conclusions}{73}{section.3.4}%
\contentsline {chapter}{\numberline {4}Low-Power Conv2D Tensor Accelerator: Hybrid 6-bit Floating-Point Computation}{75}{chapter.4}%
\contentsline {section}{\numberline {4.1}Introduction}{76}{section.4.1}%
\contentsline {section}{\numberline {4.2}Design Technique}{78}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Base embedded system architecture}{78}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Tensor processor}{78}{subsection.4.2.2}%
\contentsline {subsubsection}{\nonumberline Modes of operation}{79}{section*.100}%
\contentsline {subsubsection}{\nonumberline Dot-product with hybrid floating-point}{80}{section*.101}%
\contentsline {subsubsection}{\nonumberline Multiply-Accumulate Unit}{80}{section*.104}%
\contentsline {subsubsection}{\nonumberline On-chip memory utilization}{83}{section*.106}%
\contentsline {subsection}{\numberline {4.2.3}Training Method}{84}{subsection.4.2.3}%
\contentsline {subsubsection}{\nonumberline Training with Iterative Early Stop}{84}{section*.108}%
\contentsline {subsubsection}{\nonumberline Quantization-Aware Training}{85}{section*.109}%
\contentsline {subsection}{\numberline {4.2.4}Embedded software architecture}{87}{subsection.4.2.4}%
\contentsline {section}{\numberline {4.3}Experimental Results}{88}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Sensor Analytics Application}{88}{subsection.4.3.1}%
\contentsline {subsubsection}{\nonumberline Experimental Setup}{88}{section*.115}%
\contentsline {subsubsection}{\nonumberline Data Sets}{89}{section*.116}%
\contentsline {subsubsection}{\nonumberline CNN-Regression Model}{91}{section*.119}%
\contentsline {subsection}{\numberline {4.3.2}Training}{92}{subsection.4.3.2}%
\contentsline {subsubsection}{\nonumberline Base Model}{92}{section*.121}%
\contentsline {subsubsection}{\nonumberline TensorFlow Lite 8-bit Quantization}{93}{section*.124}%
\contentsline {subsubsection}{\nonumberline Inference of non-quantized models on HF6 hardware}{95}{section*.125}%
\contentsline {subsubsection}{\nonumberline Quantization-Aware Training for HF6 hardware}{95}{section*.126}%
\contentsline {subsubsection}{\nonumberline Quantization-Aware Training for Hybrid-Logarithmic 6-bit}{95}{section*.127}%
\contentsline {subsection}{\numberline {4.3.3}Hardware Design Exploration}{95}{subsection.4.3.3}%
\contentsline {subsubsection}{\nonumberline Benchmark on Embedded CPU}{96}{section*.128}%
\contentsline {subsubsection}{\nonumberline Benchmark on Tensor Processor Synthesized with Xilinx LogiCORE IP for Floating-Point Computation}{96}{section*.129}%
\contentsline {subsubsection}{\nonumberline Tensor Processor Synthesized with Hybrid-Float6 Hardware Architecture}{97}{section*.134}%
\contentsline {subsection}{\numberline {4.3.4}Discussion}{100}{subsection.4.3.4}%
\contentsline {subsubsection}{\nonumberline Training and Quantization}{100}{section*.136}%
\contentsline {subsubsection}{\nonumberline Implementation and Performance}{100}{section*.138}%
\contentsline {subsubsection}{\nonumberline SoC Design and Compatibility}{102}{section*.141}%
\contentsline {section}{\numberline {4.4}Conclusions}{102}{section.4.4}%
\contentsline {chapter}{\numberline {5}Conclusion and Outlook}{105}{chapter.5}%
\contentsline {section}{\numberline {5.1}State-of-the-art challenges and solutions}{105}{section.5.1}%
\contentsline {section}{\numberline {5.2}Specific Contributions}{105}{section.5.2}%
\contentsline {section}{\numberline {5.3}Future Directions}{106}{section.5.3}%
\contentsline {section}{\numberline {5.4}Final Remarks}{106}{section.5.4}%
\contentsline {chapter}{\numberline {A}Appendix}{109}{appendix.A}%
\contentsline {section}{\numberline {A.1}TensorFlow Lite Delegate Implementation and Operation}{109}{section.A.1}%
\contentsline {subsection}{\numberline {A.1.1}Implementation}{109}{subsection.A.1.1}%
\contentsline {subsection}{\numberline {A.1.2}Initialization}{110}{subsection.A.1.2}%
\contentsline {subsection}{\numberline {A.1.3}Operation}{110}{subsection.A.1.3}%
\contentsline {subsection}{\numberline {A.1.4}Collaboration diagram of TensorFlow delegate classes}{112}{subsection.A.1.4}%
\contentsline {section}{\numberline {A.2}SbS algorithm}{114}{section.A.2}%
