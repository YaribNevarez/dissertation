\@ifundefined {etoctocstyle}{\let \etoc@startlocaltoc \@gobble \let \etoc@settocdepth \@gobble \let \etoc@depthtag \@gobble \let \etoc@setlocaltop \@gobble }{}
\babel@toc {english}{}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Preamble}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}AI/ML in Industry 4.0}{1}{subsection.1.1.1}%
\contentsline {subsubsection}{\nonumberline Industry 4.0}{1}{section*.4}%
\contentsline {subsubsection}{\nonumberline Internet-of-Things in Industry}{2}{section*.5}%
\contentsline {subsection}{\numberline {1.1.2}Rationale for \gls {ai}/\gls {ml} Acceleration in \gls {iot} Applications}{2}{subsection.1.1.2}%
\contentsline {subsubsection}{\nonumberline Mission Criticality}{2}{section*.6}%
\contentsline {subsubsection}{\nonumberline Real-Time Processing}{2}{section*.7}%
\contentsline {subsubsection}{\nonumberline Data Privacy and Security}{3}{section*.8}%
\contentsline {subsubsection}{\nonumberline Offline Operation Capabilities}{3}{section*.9}%
\contentsline {subsubsection}{\nonumberline Energy Efficiency}{3}{section*.10}%
\contentsline {subsubsection}{\nonumberline Emerging Applications}{3}{section*.11}%
\contentsline {subsection}{\numberline {1.1.3}Approximation in AI/ML}{4}{subsection.1.1.3}%
\contentsline {subsubsection}{\nonumberline Network Compression and Quantization}{4}{section*.12}%
\contentsline {subsubsection}{\nonumberline Error Tolerance in AI/ML Algorithms}{5}{section*.13}%
\contentsline {subsubsection}{\nonumberline Approximate Computing}{5}{section*.14}%
\contentsline {section}{\numberline {1.2}Problem Statement}{5}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Power Dissipation}{6}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Aggressive Quantization}{6}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Interoperability}{7}{subsection.1.2.3}%
\contentsline {section}{\numberline {1.3}Working Hypothesis}{7}{section.1.3}%
\contentsline {paragraph}{\nonumberline Implications}{8}{section*.15}%
\contentsline {section}{\numberline {1.4}Research Objective}{8}{section.1.4}%
\contentsline {paragraph}{\nonumberline Core Aims}{9}{section*.16}%
\contentsline {section}{\numberline {1.5}Scope}{9}{section.1.5}%
\contentsline {section}{\numberline {1.6}Contributions}{11}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Accelerating Spike-by-Spike Neural Networks with Hybrid 8-bit Floating-Point and 4-bit Logarithmic Computation}{11}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}Accelerating Convolutional Neural Networks with Hybrid 6-bit Floating-Point Computation}{11}{subsection.1.6.2}%
\contentsline {section}{\numberline {1.7}Publications}{12}{section.1.7}%
\contentsline {section}{\numberline {1.8}Dissertation Outline}{13}{section.1.8}%
\contentsline {chapter}{\numberline {2}Background and Related Work}{15}{chapter.2}%
\contentsline {section}{\numberline {2.1}Introduction}{15}{section.2.1}%
\contentsline {section}{\numberline {2.2}Spiking Neural Networks}{16}{section.2.2}%
\contentsline {subsubsection}{\nonumberline Spike-by-Spike Neural Networks}{16}{section*.19}%
\contentsline {paragraph}{\nonumberline Basic Network Overview}{17}{section*.20}%
\contentsline {paragraph}{\nonumberline Computational Cost}{18}{section*.24}%
\contentsline {paragraph}{\nonumberline Error Tolerance}{20}{section*.26}%
\contentsline {section}{\numberline {2.3}Artificial Neural Networks}{20}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Architecture}{21}{subsection.2.3.1}%
\contentsline {subsubsection}{\nonumberline Layers}{21}{section*.28}%
\contentsline {subsubsection}{\nonumberline Weights and Bias}{21}{section*.29}%
\contentsline {subsubsection}{\nonumberline Activation Functions}{21}{section*.30}%
\contentsline {subsection}{\numberline {2.3.2}Training Process}{24}{subsection.2.3.2}%
\contentsline {subsubsection}{\nonumberline Stochastic Gradient Descent}{24}{section*.31}%
\contentsline {subsection}{\numberline {2.3.3}Multi-Layer Perceptron}{25}{subsection.2.3.3}%
\contentsline {subsubsection}{\nonumberline Key Components}{25}{section*.32}%
\contentsline {subsection}{\numberline {2.3.4}Convolutional Neural Networks}{25}{subsection.2.3.4}%
\contentsline {subsubsection}{\nonumberline Key Components}{26}{section*.33}%
\contentsline {subsubsection}{\nonumberline Conv2D Tensor Operation}{26}{section*.34}%
\contentsline {subsubsection}{\nonumberline Computational Cost of a Convolution Layer}{27}{section*.35}%
\contentsline {paragraph}{\nonumberline Definitions}{27}{section*.36}%
\contentsline {paragraph}{\nonumberline Computations Per Output Element}{27}{section*.37}%
\contentsline {paragraph}{\nonumberline Output Dimensions}{28}{section*.38}%
\contentsline {paragraph}{\nonumberline Total Computations}{28}{section*.39}%
\contentsline {subsubsection}{\nonumberline Error Tolerance in Convolution Layers}{28}{section*.40}%
\contentsline {paragraph}{\nonumberline Sources of Error}{28}{section*.41}%
\contentsline {paragraph}{\nonumberline Error Compensating Mechanisms}{28}{section*.42}%
\contentsline {paragraph}{\nonumberline Exploiting Error Tolerance for Optimization}{29}{section*.43}%
\contentsline {section}{\numberline {2.4}Neural Network Accelerators}{29}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}The Need for Accelerators}{29}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Types of Accelerators}{31}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Design Considerations}{32}{subsection.2.4.3}%
\contentsline {section}{\numberline {2.5}Precision and Effect in Training}{34}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Fixed-Point}{34}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}Floating-Point}{35}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}Post-Training Quantization}{37}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}Quantization-Aware Training}{37}{subsection.2.5.4}%
\contentsline {section}{\numberline {2.6}Dataflow Taxonomy}{39}{section.2.6}%
\contentsline {section}{\numberline {2.7}Flynn's Taxonomy}{40}{section.2.7}%
\contentsline {section}{\numberline {2.8}Multiply-Accumulate Unit}{41}{section.2.8}%
\contentsline {subsection}{\numberline {2.8.1}Design Considerations}{42}{subsection.2.8.1}%
\contentsline {section}{\numberline {2.9}Related Work}{43}{section.2.9}%
\contentsline {subsection}{\numberline {2.9.1}Aggressive Quantization}{43}{subsection.2.9.1}%
\contentsline {subsection}{\numberline {2.9.2}Spiking Neural Network Accelerators}{44}{subsection.2.9.2}%
\contentsline {subsubsection}{\nonumberline Classical Approximate Computing}{44}{section*.56}%
\contentsline {subsubsection}{\nonumberline Spike-by-Spike Neural Network Accelerators}{44}{section*.57}%
\contentsline {subsection}{\numberline {2.9.3}Convolutional Neural Network Accelerators with Custom Floating-Point Computation on \gls {fpga}}{45}{subsection.2.9.3}%
\contentsline {subsubsection}{\nonumberline Hybrid Custom Floating-Point}{45}{section*.58}%
\contentsline {subsubsection}{\nonumberline Low-Precision Floating-Point}{45}{section*.59}%
\contentsline {subsubsection}{\nonumberline Low-Power Architectures for \gls {iot} Deployments}{46}{section*.60}%
\contentsline {subsection}{\numberline {2.9.4}Neural Network Accelerators for Training and Inference with 8-bit Floating-Point Computation on \gls {asic}}{46}{subsection.2.9.4}%
\contentsline {subsection}{\numberline {2.9.5}Training Techniques with 8-bit Floating-Point Quantization}{47}{subsection.2.9.5}%
\contentsline {chapter}{\numberline {3}Acceleration with Hybrid 8-bit Floating-Point and 4-bit Logarithmic Computation}{49}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduction}{50}{section.3.1}%
\contentsline {section}{\numberline {3.2}Design Technique}{53}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Hardware Architecture}{54}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Conv Processing Unit}{55}{subsection.3.2.2}%
\contentsline {subsubsection}{\nonumberline Configuration Mode}{55}{section*.65}%
\contentsline {subsubsection}{\nonumberline Computation Mode}{56}{section*.66}%
\contentsline {subsection}{\numberline {3.2.3}Hybrid Custom Floating-Point Multiply-Accumulate Unit: Vector Dot-Product Approximation}{56}{subsection.3.2.3}%
\contentsline {subsubsection}{\nonumberline Dot-Product with Standard Floating-Point Computation}{58}{section*.68}%
\contentsline {subsubsection}{\nonumberline Dot-Product with Hybrid Custom Floating-Point and Logarithmic Computation}{59}{section*.70}%
\contentsline {section}{\numberline {3.3}Experimental Results}{62}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Performance Benchmark}{63}{subsection.3.3.1}%
\contentsline {subsubsection}{\nonumberline Benchmark on Embedded CPU}{63}{section*.73}%
\contentsline {subsubsection}{\nonumberline Benchmark on Processing Units with Standard Floating-Point Computation}{64}{section*.76}%
\contentsline {subsubsection}{\nonumberline Benchmark on Noise Tolerance Plot}{67}{section*.83}%
\contentsline {subsection}{\numberline {3.3.2}Design Exploration with Hybrid Custom Floating-Point and Logarithmic Computation}{68}{subsection.3.3.2}%
\contentsline {subsubsection}{\nonumberline Parameters for Numeric Representation of Synaptic Weight Matrix}{69}{section*.85}%
\contentsline {subsubsection}{\nonumberline Design Exploration for Dot-product with Hybrid Custom Floating-Point Computation}{70}{section*.87}%
\contentsline {subsubsection}{\nonumberline Design Exploration for Dot-Product whit Hybrid Logarithmic Computation}{70}{section*.92}%
\contentsline {subsection}{\numberline {3.3.3}Results and Discussion}{73}{subsection.3.3.3}%
\contentsline {section}{\numberline {3.4}Conclusions}{75}{section.3.4}%
\contentsline {chapter}{\numberline {4}Low-Power Conv2D Tensor Accelerator: Hybrid 6-bit Floating-Point Computation}{79}{chapter.4}%
\contentsline {section}{\numberline {4.1}Introduction}{80}{section.4.1}%
\contentsline {section}{\numberline {4.2}Design Technique}{82}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Base Embedded System Architecture}{82}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Tensor Processor}{83}{subsection.4.2.2}%
\contentsline {subsubsection}{\nonumberline Modes of Operation}{84}{section*.106}%
\contentsline {subsubsection}{\nonumberline Dot-product with Hybrid Floating-Point}{85}{section*.109}%
\contentsline {subsubsection}{\nonumberline Multiply-Accumulate Unit}{86}{section*.112}%
\contentsline {subsubsection}{\nonumberline On-chip Memory Utilization}{88}{section*.114}%
\contentsline {subsection}{\numberline {4.2.3}Training Method}{90}{subsection.4.2.3}%
\contentsline {subsubsection}{\nonumberline Training with Iterative Early Stop}{90}{section*.116}%
\contentsline {subsubsection}{\nonumberline Quantization-Aware Training}{90}{section*.117}%
\contentsline {subsection}{\numberline {4.2.4}Embedded Software Architecture}{94}{subsection.4.2.4}%
\contentsline {paragraph}{\nonumberline Model Deployment}{95}{section*.123}%
\contentsline {subsubsection}{\nonumberline TensorFlow Lite Delegate Implementation and Operation}{95}{section*.124}%
\contentsline {paragraph}{\nonumberline Implementation}{95}{section*.125}%
\contentsline {paragraph}{\nonumberline Software Classes}{95}{section*.126}%
\contentsline {paragraph}{\nonumberline Initialization}{96}{section*.128}%
\contentsline {paragraph}{\nonumberline Operation}{97}{section*.130}%
\contentsline {section}{\numberline {4.3}Experimental Results}{98}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Sensor Analytics Application}{99}{subsection.4.3.1}%
\contentsline {subsubsection}{\nonumberline Experimental Setup}{99}{section*.132}%
\contentsline {subsubsection}{\nonumberline Data Sets}{99}{section*.133}%
\contentsline {subsubsection}{\nonumberline CNN-Regression Model}{102}{section*.136}%
\contentsline {subsection}{\numberline {4.3.2}Training}{102}{subsection.4.3.2}%
\contentsline {subsubsection}{\nonumberline Base Model}{102}{section*.138}%
\contentsline {subsubsection}{\nonumberline TensorFlow Lite 8-bit Quantization}{105}{section*.141}%
\contentsline {subsubsection}{\nonumberline Inference of Non-Quantized Models on HF6 Hardware}{105}{section*.142}%
\contentsline {subsubsection}{\nonumberline Quantization-Aware Training for HF6 Hardware}{105}{section*.143}%
\contentsline {subsubsection}{\nonumberline Quantization-Aware Training for Hybrid-Logarithmic 6-bit}{106}{section*.144}%
\contentsline {subsection}{\numberline {4.3.3}Hardware Design Exploration}{106}{subsection.4.3.3}%
\contentsline {subsubsection}{\nonumberline Benchmark on Embedded CPU}{106}{section*.145}%
\contentsline {subsubsection}{\nonumberline Benchmark on Tensor Processor Synthesized with Xilinx LogiCORE IP for Floating-Point Computation}{107}{section*.146}%
\contentsline {subsubsection}{\nonumberline Tensor Processor Synthesized with Hybrid-Float6 Hardware Architecture}{108}{section*.151}%
\contentsline {subsection}{\numberline {4.3.4}Discussion}{109}{subsection.4.3.4}%
\contentsline {subsubsection}{\nonumberline Training and Quantization}{109}{section*.153}%
\contentsline {subsubsection}{\nonumberline Implementation and Performance}{111}{section*.155}%
\contentsline {subsubsection}{\nonumberline SoC Design and Compatibility}{112}{section*.158}%
\contentsline {section}{\numberline {4.4}Conclusions}{113}{section.4.4}%
\contentsline {chapter}{\numberline {5}Conclusion and Outlook}{115}{chapter.5}%
\contentsline {section}{\numberline {5.1}State-of-the-art challenges and solutions}{115}{section.5.1}%
\contentsline {section}{\numberline {5.2}Specific Contributions}{115}{section.5.2}%
\contentsline {section}{\numberline {5.3}Future Directions}{116}{section.5.3}%
\contentsline {section}{\numberline {5.4}Final Remarks}{116}{section.5.4}%
\contentsline {chapter}{\numberline {A}Appendix}{119}{appendix.A}%
\contentsline {section}{\numberline {A.1}TensorFlow Lite Delegate C++ Code Integration}{119}{section.A.1}%
\contentsline {section}{\numberline {A.2}SbS algorithm}{119}{section.A.2}%
