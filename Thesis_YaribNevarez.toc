\@ifundefined {etoctocstyle}{\let \etoc@startlocaltoc \@gobble \let \etoc@settocdepth \@gobble \let \etoc@depthtag \@gobble \let \etoc@setlocaltop \@gobble }{}
\babel@toc {english}{}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Preamble}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}AI/ML in Industry 4.0}{2}{subsection.1.1.1}%
\contentsline {subsubsection}{\nonumberline Industry 4.0}{2}{section*.4}%
\contentsline {subsubsection}{\nonumberline Internet-of-Things in Industry}{2}{section*.5}%
\contentsline {subsubsection}{\nonumberline Artificial Intelligence in Internet-of-Things}{3}{section*.6}%
\contentsline {subsection}{\numberline {1.1.2}Approximation in AI/ML}{3}{subsection.1.1.2}%
\contentsline {subsubsection}{\nonumberline Network Compression and Quantization}{3}{section*.7}%
\contentsline {subsubsection}{\nonumberline Error Tolerance in AI/ML Algorithms}{4}{section*.8}%
\contentsline {subsubsection}{\nonumberline Approximate Computing}{4}{section*.9}%
\contentsline {section}{\numberline {1.2}Problem Statement}{5}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Power Dissipation}{5}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Aggressive Quantization}{5}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Interoperability}{6}{subsection.1.2.3}%
\contentsline {section}{\numberline {1.3}Working Hypothesis}{6}{section.1.3}%
\contentsline {section}{\numberline {1.4}Research Objective}{7}{section.1.4}%
\contentsline {section}{\numberline {1.5}Scope}{8}{section.1.5}%
\contentsline {section}{\numberline {1.6}Contributions}{10}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Accelerating Spike-by-Spike Neural Networks with Hybrid 8-bit Floating-Point and 4-bit Logarithmic Computation}{10}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}Accelerating Convolutional Neural Networks with Hybrid 6-bit Floating-Point Computation}{11}{subsection.1.6.2}%
\contentsline {section}{\numberline {1.7}Publications}{11}{section.1.7}%
\contentsline {section}{\numberline {1.8}Dissertation Outline}{13}{section.1.8}%
\contentsline {chapter}{\numberline {2}Background}{15}{chapter.2}%
\contentsline {section}{\numberline {2.1}Introduction}{17}{section.2.1}%
\contentsline {section}{\numberline {2.2}Neural Networks: Fundamentals}{18}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Architecture}{18}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Nodes (Neurons)}{18}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Layers in a Neural Network}{18}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Weights and Biases}{19}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Activation Functions}{19}{subsection.2.2.5}%
\contentsline {section}{\numberline {2.3}Evolution of Neural Networks}{20}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Perceptrons}{20}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Multi-layer Perceptrons (MLP)}{20}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Deep Neural Networks (DNNs)}{20}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Convolutional Neural Networks (CNNs)}{21}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Motivation: The Limitations of CPUs for Neural Network Processing}{21}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Parallelism}{21}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Memory Bottlenecks}{21}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Energy Efficiency}{22}{subsection.2.4.3}%
\contentsline {subsection}{\numberline {2.4.4}Flexibility vs. Specialization}{22}{subsection.2.4.4}%
\contentsline {section}{\numberline {2.5}Overview of Hardware Accelerators for Neural Network Processing}{22}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Graphics Processing Units (GPUs)}{22}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}Field-Programmable Gate Arrays (FPGAs)}{23}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}Application-Specific Integrated Circuits (ASICs)}{23}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}Tensor Processing Units (TPUs)}{24}{subsection.2.5.4}%
\contentsline {section}{\numberline {2.6}The Quest for Low-Power Neural Computing}{24}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Power Reduction Techniques in Neural Network Accelerators}{24}{subsection.2.6.1}%
\contentsline {subsubsection}{\nonumberline Approximate Computing}{25}{section*.12}%
\contentsline {subsubsection}{\nonumberline Quantization}{25}{section*.13}%
\contentsline {subsubsection}{\nonumberline Custom Floating-Point Computation}{26}{section*.14}%
\contentsline {subsection}{\numberline {2.6.2}Relationship Between Computational Precision and Power Consumption}{26}{subsection.2.6.2}%
\contentsline {section}{\numberline {2.7}IEEE Standard for Floating-Point Arithmetic}{27}{section.2.7}%
\contentsline {subsection}{\numberline {2.7.1}Structure of IEEE 754 Representation}{27}{subsection.2.7.1}%
\contentsline {subsection}{\numberline {2.7.2}Benefits of IEEE 754 Standard}{28}{subsection.2.7.2}%
\contentsline {subsection}{\numberline {2.7.3}Limitations of IEEE 754 Standard}{28}{subsection.2.7.3}%
\contentsline {section}{\numberline {2.8}Errors and Limitations in Floating-Point Computation}{29}{section.2.8}%
\contentsline {subsection}{\numberline {2.8.1}Types of Floating-Point Errors}{29}{subsection.2.8.1}%
\contentsline {subsection}{\numberline {2.8.2}Implications in Neural Networks}{30}{subsection.2.8.2}%
\contentsline {section}{\numberline {2.9}Traditional Floating-Point Formats for Neural Networks}{30}{section.2.9}%
\contentsline {subsection}{\numberline {2.9.1}Inherent Inefficiencies}{30}{subsection.2.9.1}%
\contentsline {subsection}{\numberline {2.9.2}Power and Area Costs}{31}{subsection.2.9.2}%
\contentsline {subsection}{\numberline {2.9.3}Latency Concerns}{31}{subsection.2.9.3}%
\contentsline {subsection}{\numberline {2.9.4}Customization Potential}{31}{subsection.2.9.4}%
\contentsline {section}{\numberline {2.10}Custom Floating-Point Formats for Neural Network Computations}{32}{section.2.10}%
\contentsline {subsection}{\numberline {2.10.1}Trade-offs of Custom Floating-Point Formats}{32}{subsection.2.10.1}%
\contentsline {section}{\numberline {2.11}State-of-the-Art Low-Power Neural Network Accelerators}{32}{section.2.11}%
\contentsline {subsection}{\numberline {2.11.1}Accelerator Overview}{33}{subsection.2.11.1}%
\contentsline {section}{\numberline {2.12}State-of-the-Art Low-Power Accelerators with Custom Floating-Point Formats}{33}{section.2.12}%
\contentsline {subsection}{\numberline {2.12.1}Accelerator Overview}{33}{subsection.2.12.1}%
\contentsline {section}{\numberline {2.13}Gaps in Existing Solutions and Motivation for Further Exploration}{34}{section.2.13}%
\contentsline {subsection}{\numberline {2.13.1}Limitations of Current Approaches}{34}{subsection.2.13.1}%
\contentsline {subsection}{\numberline {2.13.2}Need for Further Exploration}{34}{subsection.2.13.2}%
\contentsline {subsection}{\numberline {2.13.3}Research Questions and Objectives}{35}{subsection.2.13.3}%
\contentsline {section}{\numberline {2.14}Conclusion}{35}{section.2.14}%
\contentsline {section}{\numberline {2.15}Spike-by-Spike Neural Networks}{36}{section.2.15}%
\contentsline {subsection}{\numberline {2.15.1}Basic Network Overview}{37}{subsection.2.15.1}%
\contentsline {subsection}{\numberline {2.15.2}Computational Cost}{37}{subsection.2.15.2}%
\contentsline {subsection}{\numberline {2.15.3}Error Tolerance}{38}{subsection.2.15.3}%
\contentsline {section}{\numberline {2.16}Conv2D Tensor Operation}{39}{section.2.16}%
\contentsline {section}{\numberline {2.17}Floating-point Number Representation}{40}{section.2.17}%
\contentsline {chapter}{\numberline {3}Accelerating Spike-by-Spike Neural Networks: Hybrid 8-bit Floating-Point and 4-bit Logarithmic Computation}{43}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduction}{43}{section.3.1}%
\contentsline {section}{\numberline {3.2}Related Work}{47}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Network Compression}{47}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Classical Approximate Computing}{48}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Spike-by-Spike Neural Networks Accelerators}{48}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}System Design}{49}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Hardware Architecture}{49}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Conv Processing Unit}{50}{subsection.3.3.2}%
\contentsline {subsubsection}{\nonumberline Configuration Mode}{50}{section*.24}%
\contentsline {subsubsection}{\nonumberline Computation Mode}{51}{section*.25}%
\contentsline {subsection}{\numberline {3.3.3}Dot-Product Hardware Module}{52}{subsection.3.3.3}%
\contentsline {subsubsection}{\nonumberline Dot-Product with Standard Floating-Point Computation}{53}{section*.27}%
\contentsline {subsubsection}{\nonumberline Dot-Product with Hybrid Custom Floating-Point and Logarithmic Computation}{54}{section*.29}%
\contentsline {section}{\numberline {3.4}Experimental Results}{57}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Performance Benchmark}{58}{subsection.3.4.1}%
\contentsline {subsubsection}{\nonumberline Benchmark on Embedded CPU}{58}{section*.32}%
\contentsline {subsubsection}{\nonumberline Benchmark on Processing Units with Standard Floating-Point Computation}{58}{section*.35}%
\contentsline {subsubsection}{\nonumberline Benchmark on Noise Tolerance Plot}{62}{section*.42}%
\contentsline {subsection}{\numberline {3.4.2}Design Exploration with Hybrid Custom Floating-Point and Logarithmic Approximation}{63}{subsection.3.4.2}%
\contentsline {subsubsection}{\nonumberline Parameters for Numeric Representation of Synaptic Weight Matrix}{64}{section*.44}%
\contentsline {subsubsection}{\nonumberline Design Exploration for Dot-product with Hybrid Custom Floating-Point Approximation}{64}{section*.46}%
\contentsline {subsubsection}{\nonumberline Design Exploration for Dot-Product whit Hybrid Logarithmic Approximation}{66}{section*.51}%
\contentsline {subsection}{\numberline {3.4.3}Results and Discussion}{69}{subsection.3.4.3}%
\contentsline {section}{\numberline {3.5}Conclusions}{71}{section.3.5}%
\contentsline {chapter}{\numberline {4}Accelerating Convolutional Neural Networks: Hybrid 6-bit Floating-Point Precision}{73}{chapter.4}%
\contentsline {section}{\numberline {4.1}Introduction}{74}{section.4.1}%
\contentsline {section}{\numberline {4.2}Related work}{76}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Hybrid Custom Floating-Point}{76}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Low-Precision Floating-Point}{77}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Low-Power}{77}{subsection.4.2.3}%
\contentsline {section}{\numberline {4.3}System Design}{77}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Base embedded system architecture}{77}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Tensor processor}{78}{subsection.4.3.2}%
\contentsline {subsubsection}{\nonumberline Modes of operation}{78}{section*.64}%
\contentsline {subsubsection}{\nonumberline Dot-product with hybrid floating-point}{79}{section*.65}%
\contentsline {subsubsection}{\nonumberline Multiply-Accumulate}{79}{section*.68}%
\contentsline {subsubsection}{\nonumberline On-chip memory utilization}{82}{section*.70}%
\contentsline {subsection}{\numberline {4.3.3}Training Method}{83}{subsection.4.3.3}%
\contentsline {subsubsection}{\nonumberline Training with Iterative Early Stop}{83}{section*.72}%
\contentsline {subsubsection}{\nonumberline Quantization-Aware Training}{84}{section*.73}%
\contentsline {subsection}{\numberline {4.3.4}Embedded software architecture}{84}{subsection.4.3.4}%
\contentsline {section}{\numberline {4.4}Experimental Results}{86}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Sensor Analytics Application}{86}{subsection.4.4.1}%
\contentsline {subsubsection}{\nonumberline Experimental Setup}{87}{section*.79}%
\contentsline {subsubsection}{\nonumberline Data Sets}{88}{section*.80}%
\contentsline {subsubsection}{\nonumberline CNN-Regression Model}{90}{section*.83}%
\contentsline {subsection}{\numberline {4.4.2}Training}{91}{subsection.4.4.2}%
\contentsline {subsubsection}{\nonumberline Base Model}{91}{section*.85}%
\contentsline {subsubsection}{\nonumberline TensorFlow Lite 8-bit Quantization}{92}{section*.88}%
\contentsline {subsubsection}{\nonumberline Inference of non-quantized models on HF6 hardware}{94}{section*.89}%
\contentsline {subsubsection}{\nonumberline Quantization-Aware Training for HF6 hardware}{94}{section*.90}%
\contentsline {subsubsection}{\nonumberline Quantization-Aware Training for Hybrid-Logarithmic 6-bit}{94}{section*.91}%
\contentsline {subsection}{\numberline {4.4.3}Hardware Design Exploration}{94}{subsection.4.4.3}%
\contentsline {subsubsection}{\nonumberline Benchmark on Embedded CPU}{95}{section*.92}%
\contentsline {subsubsection}{\nonumberline Benchmark on Tensor Processor Synthesized with Xilinx LogiCORE IP for Floating-Point Computation}{95}{section*.93}%
\contentsline {subsubsection}{\nonumberline Tensor Processor Synthesized with Hybrid-Float6 Hardware Architecture}{96}{section*.98}%
\contentsline {subsection}{\numberline {4.4.4}Discussion}{99}{subsection.4.4.4}%
\contentsline {subsubsection}{\nonumberline Training and Quantization}{99}{section*.100}%
\contentsline {subsubsection}{\nonumberline Implementation and Performance}{99}{section*.102}%
\contentsline {subsubsection}{\nonumberline SoC Design and Compatibility}{101}{section*.105}%
\contentsline {section}{\numberline {4.5}Conclusions}{101}{section.4.5}%
\contentsline {chapter}{\numberline {5}Conclusion and Outlook}{103}{chapter.5}%
\contentsline {section}{\numberline {5.1}Conclusion}{103}{section.5.1}%
\contentsline {section}{\numberline {5.2}Outlook}{104}{section.5.2}%
\contentsline {section}{\numberline {5.3}Summary}{105}{section.5.3}%
\contentsline {chapter}{\numberline {A}Appendix}{107}{appendix.A}%
\contentsline {section}{\numberline {A.1}SbS algorithm}{107}{section.A.1}%
