\@ifundefined {etoctocstyle}{\let \etoc@startlocaltoc \@gobble \let \etoc@settocdepth \@gobble \let \etoc@depthtag \@gobble \let \etoc@setlocaltop \@gobble }{}
\babel@toc {english}{}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Preamble}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}AI/ML in Industry 4.0}{1}{subsection.1.1.1}%
\contentsline {subsubsection}{\nonumberline Industry 4.0}{1}{section*.4}%
\contentsline {subsubsection}{\nonumberline Internet-of-Things in Industry}{2}{section*.5}%
\contentsline {subsection}{\numberline {1.1.2}Rationale for \gls {ai}/\gls {ml} Acceleration in \gls {iot} Applications}{2}{subsection.1.1.2}%
\contentsline {subsubsection}{\nonumberline Mission Criticality}{2}{section*.6}%
\contentsline {subsubsection}{\nonumberline Real-Time Processing}{2}{section*.7}%
\contentsline {subsubsection}{\nonumberline Data Privacy and Security}{3}{section*.8}%
\contentsline {subsubsection}{\nonumberline Offline Operation Capabilities}{3}{section*.9}%
\contentsline {subsubsection}{\nonumberline Energy Efficiency}{3}{section*.10}%
\contentsline {subsubsection}{\nonumberline Emerging Applications}{3}{section*.11}%
\contentsline {subsection}{\numberline {1.1.3}Approximation in AI/ML}{4}{subsection.1.1.3}%
\contentsline {subsubsection}{\nonumberline Network Compression and Quantization}{4}{section*.12}%
\contentsline {subsubsection}{\nonumberline Error Tolerance in AI/ML Algorithms}{5}{section*.13}%
\contentsline {subsubsection}{\nonumberline Approximate Computing}{5}{section*.14}%
\contentsline {section}{\numberline {1.2}Problem Statement}{5}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Power Dissipation}{6}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Aggressive Quantization}{6}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Interoperability}{7}{subsection.1.2.3}%
\contentsline {section}{\numberline {1.3}Working Hypothesis}{7}{section.1.3}%
\contentsline {paragraph}{\nonumberline Implications}{8}{section*.15}%
\contentsline {section}{\numberline {1.4}Research Objective}{8}{section.1.4}%
\contentsline {paragraph}{\nonumberline Core Aims}{9}{section*.16}%
\contentsline {section}{\numberline {1.5}Scope}{9}{section.1.5}%
\contentsline {section}{\numberline {1.6}Contributions}{11}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Accelerating Spike-by-Spike Neural Networks with Hybrid 8-bit Floating-Point and 4-bit Logarithmic Computation}{11}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}Accelerating Convolutional Neural Networks with Hybrid 6-bit Floating-Point Computation}{11}{subsection.1.6.2}%
\contentsline {section}{\numberline {1.7}Publications}{12}{section.1.7}%
\contentsline {section}{\numberline {1.8}Dissertation Outline}{13}{section.1.8}%
\contentsline {chapter}{\numberline {2}Background and Related Work}{15}{chapter.2}%
\contentsline {section}{\numberline {2.1}Introduction}{15}{section.2.1}%
\contentsline {section}{\numberline {2.2}Spiking Neural Networks}{16}{section.2.2}%
\contentsline {subsubsection}{\nonumberline Spike-by-Spike Neural Networks}{16}{section*.19}%
\contentsline {paragraph}{\nonumberline Basic Network Overview}{17}{section*.20}%
\contentsline {paragraph}{\nonumberline Computational Cost}{18}{section*.24}%
\contentsline {paragraph}{\nonumberline Error Tolerance}{20}{section*.26}%
\contentsline {section}{\numberline {2.3}Artificial Neural Networks}{20}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Architecture}{21}{subsection.2.3.1}%
\contentsline {subsubsection}{\nonumberline Layers}{21}{section*.28}%
\contentsline {subsubsection}{\nonumberline Weights and Bias}{21}{section*.29}%
\contentsline {subsubsection}{\nonumberline Activation Functions}{21}{section*.30}%
\contentsline {subsection}{\numberline {2.3.2}Training Process}{24}{subsection.2.3.2}%
\contentsline {subsubsection}{\nonumberline Stochastic Gradient Descent}{24}{section*.31}%
\contentsline {subsection}{\numberline {2.3.3}Multi-Layer Perceptron}{25}{subsection.2.3.3}%
\contentsline {subsubsection}{\nonumberline Key Components}{25}{section*.32}%
\contentsline {subsection}{\numberline {2.3.4}Convolutional Neural Networks}{25}{subsection.2.3.4}%
\contentsline {subsubsection}{\nonumberline Key Components}{26}{section*.33}%
\contentsline {subsubsection}{\nonumberline Conv2D Tensor Operation}{26}{section*.34}%
\contentsline {subsubsection}{\nonumberline Computational Cost of a Convolution Layer}{27}{section*.35}%
\contentsline {paragraph}{\nonumberline Definitions}{27}{section*.36}%
\contentsline {paragraph}{\nonumberline Computations Per Output Element}{27}{section*.37}%
\contentsline {paragraph}{\nonumberline Output Dimensions}{28}{section*.38}%
\contentsline {paragraph}{\nonumberline Total Computations}{28}{section*.39}%
\contentsline {subsubsection}{\nonumberline Error Tolerance in Convolution Layers}{28}{section*.40}%
\contentsline {paragraph}{\nonumberline Sources of Error}{28}{section*.41}%
\contentsline {paragraph}{\nonumberline Error Compensating Mechanisms}{28}{section*.42}%
\contentsline {paragraph}{\nonumberline Exploiting Error Tolerance for Optimization}{29}{section*.43}%
\contentsline {section}{\numberline {2.4}Neural Network Accelerators}{29}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}The Need for Accelerators}{29}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Types of Accelerators}{31}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Design Considerations}{32}{subsection.2.4.3}%
\contentsline {section}{\numberline {2.5}Precision and Effect in Training}{34}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Fixed-Point}{34}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}Floating-Point}{35}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}Post-Training Quantization}{37}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}Quantization-Aware Training}{37}{subsection.2.5.4}%
\contentsline {section}{\numberline {2.6}Dataflow Taxonomy}{39}{section.2.6}%
\contentsline {section}{\numberline {2.7}Flynn's Taxonomy}{40}{section.2.7}%
\contentsline {section}{\numberline {2.8}Multiply-Accumulate Unit}{41}{section.2.8}%
\contentsline {subsection}{\numberline {2.8.1}Design Considerations}{42}{subsection.2.8.1}%
\contentsline {section}{\numberline {2.9}Related Work}{43}{section.2.9}%
\contentsline {subsection}{\numberline {2.9.1}Aggressive Quantization}{43}{subsection.2.9.1}%
\contentsline {subsection}{\numberline {2.9.2}Spiking Neural Network Accelerators}{44}{subsection.2.9.2}%
\contentsline {subsubsection}{\nonumberline Classical Approximate Computing}{44}{section*.56}%
\contentsline {subsubsection}{\nonumberline Spike-by-Spike Neural Network Accelerators}{44}{section*.57}%
\contentsline {subsection}{\numberline {2.9.3}Convolutional Neural Network Accelerators with Custom Floating-Point Computation on \gls {fpga}}{45}{subsection.2.9.3}%
\contentsline {subsubsection}{\nonumberline Hybrid Custom Floating-Point}{45}{section*.58}%
\contentsline {subsubsection}{\nonumberline Low-Precision Floating-Point}{46}{section*.60}%
\contentsline {subsubsection}{\nonumberline Low-Power Architectures for \gls {iot} Deployments}{47}{section*.63}%
\contentsline {subsection}{\numberline {2.9.4}Neural Network Accelerators for Training and Inference with 8-bit Floating-Point Computation on \gls {asic}}{48}{subsection.2.9.4}%
\contentsline {subsection}{\numberline {2.9.5}Training Techniques with 8-bit Floating-Point Quantization}{49}{subsection.2.9.5}%
\contentsline {chapter}{\numberline {3}Acceleration with Hybrid 8-bit Floating-Point and 4-bit Logarithmic Computation}{51}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduction}{52}{section.3.1}%
\contentsline {section}{\numberline {3.2}Design Technique}{55}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Hardware Architecture}{56}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Conv Processing Unit}{57}{subsection.3.2.2}%
\contentsline {subsubsection}{\nonumberline Configuration Mode}{57}{section*.69}%
\contentsline {subsubsection}{\nonumberline Computation Mode}{58}{section*.70}%
\contentsline {subsection}{\numberline {3.2.3}Hybrid Custom Floating-Point Multiply-Accumulate Unit: Vector Dot-Product Approximation}{58}{subsection.3.2.3}%
\contentsline {subsubsection}{\nonumberline Dot-Product with Standard Floating-Point Computation}{60}{section*.72}%
\contentsline {subsubsection}{\nonumberline Dot-Product with Hybrid Custom Floating-Point and Logarithmic Computation}{61}{section*.74}%
\contentsline {section}{\numberline {3.3}Experimental Results}{64}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Performance Benchmark}{65}{subsection.3.3.1}%
\contentsline {subsubsection}{\nonumberline Benchmark on Embedded CPU}{65}{section*.77}%
\contentsline {subsubsection}{\nonumberline Benchmark on Processing Units with Standard Floating-Point Computation}{66}{section*.80}%
\contentsline {subsubsection}{\nonumberline Benchmark on Noise Tolerance Plot}{69}{section*.87}%
\contentsline {subsection}{\numberline {3.3.2}Design Exploration with Hybrid Custom Floating-Point and Logarithmic Computation}{70}{subsection.3.3.2}%
\contentsline {subsubsection}{\nonumberline Parameters for Numeric Representation of Synaptic Weight Matrix}{71}{section*.89}%
\contentsline {subsubsection}{\nonumberline Design Exploration for Dot-product with Hybrid Custom Floating-Point Computation}{72}{section*.91}%
\contentsline {subsubsection}{\nonumberline Design Exploration for Dot-Product whit Hybrid Logarithmic Computation}{72}{section*.96}%
\contentsline {subsection}{\numberline {3.3.3}Results and Discussion}{75}{subsection.3.3.3}%
\contentsline {section}{\numberline {3.4}Conclusions}{77}{section.3.4}%
\contentsline {chapter}{\numberline {4}Low-Power Conv2D Tensor Accelerator: Hybrid 6-bit Floating-Point Computation}{81}{chapter.4}%
\contentsline {section}{\numberline {4.1}Introduction}{82}{section.4.1}%
\contentsline {section}{\numberline {4.2}Design Technique}{84}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Base Embedded System Architecture}{84}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Tensor Processor}{85}{subsection.4.2.2}%
\contentsline {subsubsection}{\nonumberline Modes of Operation}{86}{section*.110}%
\contentsline {subsubsection}{\nonumberline Dot-product with Hybrid Floating-Point}{87}{section*.113}%
\contentsline {subsubsection}{\nonumberline Multiply-Accumulate Unit}{88}{section*.116}%
\contentsline {subsubsection}{\nonumberline On-chip Memory Utilization}{90}{section*.118}%
\contentsline {subsection}{\numberline {4.2.3}Training Method}{92}{subsection.4.2.3}%
\contentsline {subsubsection}{\nonumberline Training with Iterative Early Stop}{92}{section*.120}%
\contentsline {subsubsection}{\nonumberline Quantization-Aware Training}{92}{section*.121}%
\contentsline {subsection}{\numberline {4.2.4}Embedded Software Architecture}{96}{subsection.4.2.4}%
\contentsline {paragraph}{\nonumberline Model Deployment}{97}{section*.127}%
\contentsline {subsubsection}{\nonumberline TensorFlow Lite Delegate Implementation and Operation}{97}{section*.128}%
\contentsline {paragraph}{\nonumberline Implementation}{97}{section*.129}%
\contentsline {paragraph}{\nonumberline Software Classes}{97}{section*.130}%
\contentsline {paragraph}{\nonumberline Initialization}{98}{section*.132}%
\contentsline {paragraph}{\nonumberline Operation}{99}{section*.134}%
\contentsline {section}{\numberline {4.3}Experimental Results}{100}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Sensor Analytics Application}{101}{subsection.4.3.1}%
\contentsline {subsubsection}{\nonumberline Experimental Setup}{101}{section*.136}%
\contentsline {subsubsection}{\nonumberline Data Sets}{101}{section*.137}%
\contentsline {subsubsection}{\nonumberline CNN-Regression Model}{104}{section*.140}%
\contentsline {subsection}{\numberline {4.3.2}Training}{104}{subsection.4.3.2}%
\contentsline {subsubsection}{\nonumberline Base Model}{104}{section*.142}%
\contentsline {subsubsection}{\nonumberline TensorFlow Lite 8-bit Quantization}{107}{section*.145}%
\contentsline {subsubsection}{\nonumberline Inference of Non-Quantized Models on HF6 Hardware}{107}{section*.146}%
\contentsline {subsubsection}{\nonumberline Quantization-Aware Training for HF6 Hardware}{107}{section*.147}%
\contentsline {subsubsection}{\nonumberline Quantization-Aware Training for Hybrid-Logarithmic 6-bit}{108}{section*.148}%
\contentsline {subsection}{\numberline {4.3.3}Hardware Design Exploration}{108}{subsection.4.3.3}%
\contentsline {subsubsection}{\nonumberline Benchmark on Embedded CPU}{108}{section*.149}%
\contentsline {subsubsection}{\nonumberline Benchmark on Tensor Processor Synthesized with Xilinx LogiCORE IP for Floating-Point Computation}{109}{section*.150}%
\contentsline {subsubsection}{\nonumberline Tensor Processor Synthesized with Hybrid-Float6 Hardware Architecture}{110}{section*.155}%
\contentsline {subsection}{\numberline {4.3.4}Discussion}{111}{subsection.4.3.4}%
\contentsline {subsubsection}{\nonumberline Training and Quantization}{111}{section*.157}%
\contentsline {subsubsection}{\nonumberline Implementation and Performance}{113}{section*.159}%
\contentsline {subsubsection}{\nonumberline SoC Design and Compatibility}{114}{section*.162}%
\contentsline {section}{\numberline {4.4}Conclusions}{115}{section.4.4}%
\contentsline {chapter}{\numberline {5}Conclusion and Outlook}{117}{chapter.5}%
\contentsline {section}{\numberline {5.1}State-of-the-art challenges and solutions}{117}{section.5.1}%
\contentsline {section}{\numberline {5.2}Specific Contributions}{117}{section.5.2}%
\contentsline {section}{\numberline {5.3}Future Directions}{118}{section.5.3}%
\contentsline {section}{\numberline {5.4}Final Remarks}{118}{section.5.4}%
\contentsline {chapter}{\numberline {A}Appendix}{121}{appendix.A}%
\contentsline {section}{\numberline {A.1}Tensor Processor Delegate and Hardware Drivers}{121}{section.A.1}%
\contentsline {subsection}{\numberline {A.1.1}Tensor Processor Delegate}{122}{subsection.A.1.1}%
\contentsline {subsection}{\numberline {A.1.2}Hardware Drivers}{123}{subsection.A.1.2}%
\contentsline {subsection}{\numberline {A.1.3}ARM Generic Interrupt Controller}{124}{subsection.A.1.3}%
\contentsline {subsection}{\numberline {A.1.4}Supporting Classes}{124}{subsection.A.1.4}%
\contentsline {section}{\numberline {A.2}TensorFlow Lite Integration}{125}{section.A.2}%
\contentsline {section}{\numberline {A.3}SbS algorithm}{138}{section.A.3}%
